{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ovXQweBDuGy_",
        "outputId": "876dffa9-7b28-40a0-879f-f8370d558b7c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: streamlink in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (6.9.0)\n",
            "Requirement already satisfied: certifi in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (2024.7.4)\n",
            "Requirement already satisfied: isodate in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (0.6.1)\n",
            "Requirement already satisfied: lxml<6,>=4.6.4 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (5.2.2)\n",
            "Requirement already satisfied: pycountry in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (24.6.1)\n",
            "Requirement already satisfied: pycryptodome<4,>=3.4.3 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (3.20.0)\n",
            "Requirement already satisfied: PySocks!=1.5.7,>=1.5.6 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (1.7.1)\n",
            "Requirement already satisfied: requests<3,>=2.26.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (2.32.3)\n",
            "Requirement already satisfied: trio<1,>=0.22.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (0.26.2)\n",
            "Requirement already satisfied: trio-websocket<1,>=0.9.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (0.11.1)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (4.12.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.26.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (2.2.2)\n",
            "Requirement already satisfied: websocket-client<2,>=1.2.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from streamlink) (1.8.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.26.0->streamlink) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.26.0->streamlink) (3.7)\n",
            "Requirement already satisfied: attrs>=23.2.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1,>=0.22.0->streamlink) (24.2.0)\n",
            "Requirement already satisfied: sortedcontainers in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1,>=0.22.0->streamlink) (2.4.0)\n",
            "Requirement already satisfied: outcome in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1,>=0.22.0->streamlink) (1.3.0.post0)\n",
            "Requirement already satisfied: sniffio>=1.3.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1,>=0.22.0->streamlink) (1.3.1)\n",
            "Requirement already satisfied: cffi>=1.14 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio<1,>=0.22.0->streamlink) (1.17.1)\n",
            "Requirement already satisfied: wsproto>=0.14 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from trio-websocket<1,>=0.9.0->streamlink) (1.2.0)\n",
            "Requirement already satisfied: six in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from isodate->streamlink) (1.16.0)\n",
            "Requirement already satisfied: pycparser in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from cffi>=1.14->trio<1,>=0.22.0->streamlink) (2.22)\n",
            "Requirement already satisfied: h11<1,>=0.9.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from wsproto>=0.14->trio-websocket<1,>=0.9.0->streamlink) (0.14.0)\n",
            "Requirement already satisfied: ultralytics in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (8.2.87)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (3.9.1)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=7.1.2 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (10.4.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (1.14.0)\n",
            "Requirement already satisfied: torch>=1.8.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.3.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.18.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (4.66.5)\n",
            "Requirement already satisfied: psutil in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from ultralytics) (6.0.0)\n",
            "Requirement already satisfied: py-cpuinfo in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.2.2)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (0.13.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from ultralytics) (2.0.6)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.5)\n",
            "Requirement already satisfied: packaging>=20.0 in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas>=1.1.4->ultralytics) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests>=2.23.0->ultralytics) (2024.7.4)\n",
            "Requirement already satisfied: filelock in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (4.12.2)\n",
            "Requirement already satisfied: sympy in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (1.13.2)\n",
            "Requirement already satisfied: networkx in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
            "Requirement already satisfied: jinja2 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
            "Requirement already satisfied: fsspec in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
            "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.8.0->ultralytics) (2021.4.0)\n",
            "Requirement already satisfied: colorama in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from tqdm>=4.64.0->ultralytics) (0.4.6)\n",
            "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.4.0)\n",
            "Requirement already satisfied: tbb==2021.* in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=1.8.0->ultralytics) (2021.13.1)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\saad9\\appdata\\roaming\\python\\python312\\site-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\saad9\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.8.0->ultralytics) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlink\n",
        "!pip install ultralytics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "HJuQCNQdDidA"
      },
      "outputs": [],
      "source": [
        "NYC_video = '/content/drive/MyDrive/Panic in Times Square After Nearby Manhole Explosion (Courtesy of EarthCam - April 10, 2022).mp4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ONrvBjjND4wr",
        "outputId": "3310909c-85a6-4e91-d370-d3bcd466bb3a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracted extracted_frames/frame_0.jpg\n",
            "Extracted extracted_frames/frame_30.jpg\n",
            "Extracted extracted_frames/frame_60.jpg\n",
            "Extracted extracted_frames/frame_90.jpg\n",
            "Extracted extracted_frames/frame_120.jpg\n",
            "Extracted extracted_frames/frame_150.jpg\n",
            "Extracted extracted_frames/frame_180.jpg\n",
            "Extracted extracted_frames/frame_210.jpg\n",
            "Extracted extracted_frames/frame_240.jpg\n",
            "Extracted extracted_frames/frame_270.jpg\n",
            "Extracted extracted_frames/frame_300.jpg\n",
            "Extracted extracted_frames/frame_330.jpg\n",
            "Extracted extracted_frames/frame_360.jpg\n",
            "Extracted extracted_frames/frame_390.jpg\n",
            "Extracted extracted_frames/frame_420.jpg\n",
            "Extracted extracted_frames/frame_450.jpg\n",
            "Extracted extracted_frames/frame_480.jpg\n",
            "Extracted extracted_frames/frame_510.jpg\n",
            "Extracted extracted_frames/frame_540.jpg\n",
            "Extracted extracted_frames/frame_570.jpg\n",
            "Extracted extracted_frames/frame_600.jpg\n",
            "Extracted extracted_frames/frame_630.jpg\n",
            "Extracted extracted_frames/frame_660.jpg\n",
            "Extracted extracted_frames/frame_690.jpg\n",
            "Extracted extracted_frames/frame_720.jpg\n",
            "Extracted extracted_frames/frame_750.jpg\n",
            "Extracted extracted_frames/frame_780.jpg\n",
            "Extracted extracted_frames/frame_810.jpg\n",
            "Extracted extracted_frames/frame_840.jpg\n",
            "Extracted extracted_frames/frame_870.jpg\n",
            "Extracted extracted_frames/frame_900.jpg\n",
            "Extracted extracted_frames/frame_930.jpg\n",
            "Extracted extracted_frames/frame_960.jpg\n",
            "Extracted extracted_frames/frame_990.jpg\n",
            "Extracted extracted_frames/frame_1020.jpg\n",
            "Extracted extracted_frames/frame_1050.jpg\n",
            "Extracted extracted_frames/frame_1080.jpg\n",
            "Extracted extracted_frames/frame_1110.jpg\n",
            "Extracted extracted_frames/frame_1140.jpg\n",
            "Extracted extracted_frames/frame_1170.jpg\n",
            "Extracted extracted_frames/frame_1200.jpg\n",
            "Extracted extracted_frames/frame_1230.jpg\n",
            "Extracted extracted_frames/frame_1260.jpg\n",
            "Extracted extracted_frames/frame_1290.jpg\n",
            "Extracted extracted_frames/frame_1320.jpg\n",
            "Extracted extracted_frames/frame_1350.jpg\n",
            "Extracted extracted_frames/frame_1380.jpg\n",
            "Extracted extracted_frames/frame_1410.jpg\n",
            "Extracted extracted_frames/frame_1440.jpg\n",
            "Extracted extracted_frames/frame_1470.jpg\n",
            "Extracted extracted_frames/frame_1500.jpg\n",
            "Extracted extracted_frames/frame_1530.jpg\n",
            "Extracted extracted_frames/frame_1560.jpg\n",
            "Extracted extracted_frames/frame_1590.jpg\n",
            "Extracted extracted_frames/frame_1620.jpg\n",
            "Extracted extracted_frames/frame_1650.jpg\n",
            "Extracted extracted_frames/frame_1680.jpg\n",
            "Extracted extracted_frames/frame_1710.jpg\n",
            "Extracted extracted_frames/frame_1740.jpg\n",
            "Extracted extracted_frames/frame_1770.jpg\n",
            "Extracted extracted_frames/frame_1800.jpg\n",
            "Extracted extracted_frames/frame_1830.jpg\n",
            "Extracted extracted_frames/frame_1860.jpg\n",
            "Extracted extracted_frames/frame_1890.jpg\n",
            "Extracted extracted_frames/frame_1920.jpg\n",
            "Extracted extracted_frames/frame_1950.jpg\n",
            "Extracted extracted_frames/frame_1980.jpg\n",
            "Extracted extracted_frames/frame_2010.jpg\n",
            "Extracted extracted_frames/frame_2040.jpg\n",
            "Extracted extracted_frames/frame_2070.jpg\n",
            "Extracted extracted_frames/frame_2100.jpg\n",
            "Extracted extracted_frames/frame_2130.jpg\n",
            "Extracted extracted_frames/frame_2160.jpg\n",
            "Extracted extracted_frames/frame_2190.jpg\n",
            "Extracted extracted_frames/frame_2220.jpg\n",
            "Extracted extracted_frames/frame_2250.jpg\n",
            "Extracted extracted_frames/frame_2280.jpg\n",
            "Extracted extracted_frames/frame_2310.jpg\n",
            "Extracted extracted_frames/frame_2340.jpg\n",
            "Extracted extracted_frames/frame_2370.jpg\n",
            "Extracted extracted_frames/frame_2400.jpg\n",
            "Extracted extracted_frames/frame_2430.jpg\n",
            "Extracted extracted_frames/frame_2460.jpg\n",
            "Extracted extracted_frames/frame_2490.jpg\n",
            "Extracted extracted_frames/frame_2520.jpg\n",
            "Extracted extracted_frames/frame_2550.jpg\n",
            "Extracted extracted_frames/frame_2580.jpg\n",
            "Extracted extracted_frames/frame_2610.jpg\n",
            "Extracted extracted_frames/frame_2640.jpg\n",
            "Extracted extracted_frames/frame_2670.jpg\n",
            "Extracted extracted_frames/frame_2700.jpg\n",
            "Extracted extracted_frames/frame_2730.jpg\n",
            "Extracted extracted_frames/frame_2760.jpg\n",
            "Extracted extracted_frames/frame_2790.jpg\n",
            "Extracted extracted_frames/frame_2820.jpg\n",
            "Extracted extracted_frames/frame_2850.jpg\n",
            "Extracted extracted_frames/frame_2880.jpg\n",
            "Extracted extracted_frames/frame_2910.jpg\n",
            "Extracted extracted_frames/frame_2940.jpg\n",
            "Extracted extracted_frames/frame_2970.jpg\n",
            "Extracted extracted_frames/frame_3000.jpg\n",
            "Extracted extracted_frames/frame_3030.jpg\n",
            "Extracted extracted_frames/frame_3060.jpg\n",
            "Extracted extracted_frames/frame_3090.jpg\n",
            "Extracted extracted_frames/frame_3120.jpg\n",
            "Extracted extracted_frames/frame_3150.jpg\n",
            "Extracted extracted_frames/frame_3180.jpg\n",
            "Extracted extracted_frames/frame_3210.jpg\n",
            "Extracted extracted_frames/frame_3240.jpg\n",
            "Extracted extracted_frames/frame_3270.jpg\n",
            "Extracted extracted_frames/frame_3300.jpg\n",
            "Extracted extracted_frames/frame_3330.jpg\n",
            "Extracted extracted_frames/frame_3360.jpg\n",
            "Extracted extracted_frames/frame_3390.jpg\n",
            "Extracted extracted_frames/frame_3420.jpg\n",
            "Extracted extracted_frames/frame_3450.jpg\n",
            "Extracted extracted_frames/frame_3480.jpg\n",
            "Extracted extracted_frames/frame_3510.jpg\n",
            "Extracted extracted_frames/frame_3540.jpg\n",
            "Extracted extracted_frames/frame_3570.jpg\n",
            "Extracted extracted_frames/frame_3600.jpg\n",
            "Extracted extracted_frames/frame_3630.jpg\n",
            "Extracted extracted_frames/frame_3660.jpg\n",
            "Extracted extracted_frames/frame_3690.jpg\n",
            "Extracted extracted_frames/frame_3720.jpg\n",
            "Extracted extracted_frames/frame_3750.jpg\n",
            "Extracted extracted_frames/frame_3780.jpg\n",
            "Extracted extracted_frames/frame_3810.jpg\n",
            "Extracted extracted_frames/frame_3840.jpg\n",
            "Extracted extracted_frames/frame_3870.jpg\n",
            "Extracted extracted_frames/frame_3900.jpg\n",
            "Extracted extracted_frames/frame_3930.jpg\n",
            "Extracted extracted_frames/frame_3960.jpg\n",
            "Extracted extracted_frames/frame_3990.jpg\n",
            "Extracted extracted_frames/frame_4020.jpg\n",
            "Extracted extracted_frames/frame_4050.jpg\n",
            "Extracted extracted_frames/frame_4080.jpg\n",
            "Extracted extracted_frames/frame_4110.jpg\n",
            "Extracted extracted_frames/frame_4140.jpg\n",
            "Extracted extracted_frames/frame_4170.jpg\n",
            "Extracted extracted_frames/frame_4200.jpg\n",
            "Extracted extracted_frames/frame_4230.jpg\n",
            "Extracted extracted_frames/frame_4260.jpg\n",
            "Extracted extracted_frames/frame_4290.jpg\n",
            "Extracted extracted_frames/frame_4320.jpg\n",
            "Extracted extracted_frames/frame_4350.jpg\n",
            "Extracted extracted_frames/frame_4380.jpg\n",
            "Extracted extracted_frames/frame_4410.jpg\n",
            "Extracted extracted_frames/frame_4440.jpg\n",
            "Extracted extracted_frames/frame_4470.jpg\n",
            "Extracted extracted_frames/frame_4500.jpg\n",
            "Extracted extracted_frames/frame_4530.jpg\n",
            "Extracted extracted_frames/frame_4560.jpg\n",
            "Extracted extracted_frames/frame_4590.jpg\n",
            "Extracted extracted_frames/frame_4620.jpg\n",
            "Extracted extracted_frames/frame_4650.jpg\n",
            "Extracted extracted_frames/frame_4680.jpg\n",
            "Extracted extracted_frames/frame_4710.jpg\n",
            "Extracted extracted_frames/frame_4740.jpg\n",
            "Extracted extracted_frames/frame_4770.jpg\n",
            "Extracted extracted_frames/frame_4800.jpg\n",
            "Extracted extracted_frames/frame_4830.jpg\n",
            "Extracted extracted_frames/frame_4860.jpg\n",
            "Extracted extracted_frames/frame_4890.jpg\n",
            "Extracted extracted_frames/frame_4920.jpg\n",
            "Extracted extracted_frames/frame_4950.jpg\n",
            "Extracted extracted_frames/frame_4980.jpg\n",
            "Extracted extracted_frames/frame_5010.jpg\n",
            "Extracted extracted_frames/frame_5040.jpg\n",
            "Extracted extracted_frames/frame_5070.jpg\n",
            "Extracted extracted_frames/frame_5100.jpg\n",
            "Extracted extracted_frames/frame_5130.jpg\n",
            "Extracted extracted_frames/frame_5160.jpg\n",
            "Extracted extracted_frames/frame_5190.jpg\n",
            "Extracted extracted_frames/frame_5220.jpg\n",
            "Extracted extracted_frames/frame_5250.jpg\n",
            "Extracted extracted_frames/frame_5280.jpg\n",
            "Extracted extracted_frames/frame_5310.jpg\n",
            "Extracted extracted_frames/frame_5340.jpg\n",
            "Extracted extracted_frames/frame_5370.jpg\n",
            "Extracted extracted_frames/frame_5400.jpg\n",
            "Extracted extracted_frames/frame_5430.jpg\n",
            "Extracted extracted_frames/frame_5460.jpg\n",
            "Extracted extracted_frames/frame_5490.jpg\n",
            "Extracted extracted_frames/frame_5520.jpg\n",
            "Extracted extracted_frames/frame_5550.jpg\n",
            "Extracted extracted_frames/frame_5580.jpg\n",
            "Extracted extracted_frames/frame_5610.jpg\n",
            "Extracted extracted_frames/frame_5640.jpg\n",
            "Extracted extracted_frames/frame_5670.jpg\n",
            "Extracted extracted_frames/frame_5700.jpg\n",
            "Extracted extracted_frames/frame_5730.jpg\n",
            "Extracted extracted_frames/frame_5760.jpg\n",
            "Extracted extracted_frames/frame_5790.jpg\n",
            "Extracted extracted_frames/frame_5820.jpg\n",
            "Extracted extracted_frames/frame_5850.jpg\n",
            "Extracted extracted_frames/frame_5880.jpg\n",
            "Extracted extracted_frames/frame_5910.jpg\n",
            "Extracted extracted_frames/frame_5940.jpg\n",
            "Extracted extracted_frames/frame_5970.jpg\n",
            "Extracted extracted_frames/frame_6000.jpg\n",
            "Extracted extracted_frames/frame_6030.jpg\n",
            "Extracted extracted_frames/frame_6060.jpg\n",
            "Extracted extracted_frames/frame_6090.jpg\n",
            "Extracted extracted_frames/frame_6120.jpg\n",
            "Extracted extracted_frames/frame_6150.jpg\n",
            "Extracted extracted_frames/frame_6180.jpg\n",
            "Extracted extracted_frames/frame_6210.jpg\n",
            "Extracted extracted_frames/frame_6240.jpg\n",
            "Extracted extracted_frames/frame_6270.jpg\n",
            "Extracted extracted_frames/frame_6300.jpg\n",
            "Extracted extracted_frames/frame_6330.jpg\n",
            "Extracted extracted_frames/frame_6360.jpg\n",
            "Extracted extracted_frames/frame_6390.jpg\n",
            "Extracted extracted_frames/frame_6420.jpg\n",
            "Extracted extracted_frames/frame_6450.jpg\n",
            "Extracted extracted_frames/frame_6480.jpg\n",
            "Extracted extracted_frames/frame_6510.jpg\n",
            "Extracted extracted_frames/frame_6540.jpg\n",
            "Extracted extracted_frames/frame_6570.jpg\n",
            "Extracted extracted_frames/frame_6600.jpg\n",
            "Extracted extracted_frames/frame_6630.jpg\n",
            "Extracted extracted_frames/frame_6660.jpg\n",
            "Extracted extracted_frames/frame_6690.jpg\n",
            "Extracted extracted_frames/frame_6720.jpg\n",
            "Extracted extracted_frames/frame_6750.jpg\n",
            "Extracted extracted_frames/frame_6780.jpg\n",
            "Extracted extracted_frames/frame_6810.jpg\n",
            "Extracted extracted_frames/frame_6840.jpg\n",
            "Extracted extracted_frames/frame_6870.jpg\n",
            "Extracted extracted_frames/frame_6900.jpg\n",
            "Extracted extracted_frames/frame_6930.jpg\n",
            "Extracted extracted_frames/frame_6960.jpg\n",
            "Extracted extracted_frames/frame_6990.jpg\n",
            "Extracted extracted_frames/frame_7020.jpg\n",
            "Extracted extracted_frames/frame_7050.jpg\n",
            "Extracted extracted_frames/frame_7080.jpg\n",
            "Extracted extracted_frames/frame_7110.jpg\n",
            "Extracted extracted_frames/frame_7140.jpg\n",
            "Extracted extracted_frames/frame_7170.jpg\n",
            "Extracted extracted_frames/frame_7200.jpg\n",
            "Extracted extracted_frames/frame_7230.jpg\n",
            "Extracted extracted_frames/frame_7260.jpg\n",
            "Extracted extracted_frames/frame_7290.jpg\n",
            "Extracted extracted_frames/frame_7320.jpg\n",
            "Extracted extracted_frames/frame_7350.jpg\n",
            "Extracted extracted_frames/frame_7380.jpg\n",
            "Extracted extracted_frames/frame_7410.jpg\n",
            "Extracted extracted_frames/frame_7440.jpg\n",
            "Extracted extracted_frames/frame_7470.jpg\n",
            "Extracted extracted_frames/frame_7500.jpg\n",
            "Extracted extracted_frames/frame_7530.jpg\n",
            "Extracted extracted_frames/frame_7560.jpg\n",
            "Extracted extracted_frames/frame_7590.jpg\n",
            "Extracted extracted_frames/frame_7620.jpg\n",
            "Extracted extracted_frames/frame_7650.jpg\n",
            "Extracted extracted_frames/frame_7680.jpg\n",
            "Extracted extracted_frames/frame_7710.jpg\n",
            "Extracted extracted_frames/frame_7740.jpg\n",
            "Extracted extracted_frames/frame_7770.jpg\n",
            "Extracted extracted_frames/frame_7800.jpg\n",
            "Extracted extracted_frames/frame_7830.jpg\n",
            "Extracted extracted_frames/frame_7860.jpg\n",
            "Extracted extracted_frames/frame_7890.jpg\n",
            "Extracted extracted_frames/frame_7920.jpg\n",
            "Extracted extracted_frames/frame_7950.jpg\n",
            "Extracted extracted_frames/frame_7980.jpg\n",
            "Extracted extracted_frames/frame_8010.jpg\n",
            "Extracted extracted_frames/frame_8040.jpg\n",
            "Extracted extracted_frames/frame_8070.jpg\n",
            "Extracted extracted_frames/frame_8100.jpg\n",
            "Extracted extracted_frames/frame_8130.jpg\n",
            "Extracted extracted_frames/frame_8160.jpg\n",
            "Extracted extracted_frames/frame_8190.jpg\n",
            "Extracted extracted_frames/frame_8220.jpg\n",
            "Extracted extracted_frames/frame_8250.jpg\n",
            "Extracted extracted_frames/frame_8280.jpg\n",
            "Extracted extracted_frames/frame_8310.jpg\n",
            "Extracted extracted_frames/frame_8340.jpg\n",
            "Extracted extracted_frames/frame_8370.jpg\n",
            "Extracted extracted_frames/frame_8400.jpg\n",
            "Extracted extracted_frames/frame_8430.jpg\n",
            "Extracted extracted_frames/frame_8460.jpg\n",
            "Extracted extracted_frames/frame_8490.jpg\n",
            "Extracted extracted_frames/frame_8520.jpg\n",
            "Extracted extracted_frames/frame_8550.jpg\n",
            "Extracted extracted_frames/frame_8580.jpg\n",
            "Extracted extracted_frames/frame_8610.jpg\n",
            "Extracted extracted_frames/frame_8640.jpg\n",
            "Extracted extracted_frames/frame_8670.jpg\n",
            "Extracted extracted_frames/frame_8700.jpg\n",
            "Extracted extracted_frames/frame_8730.jpg\n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "def extract_frames(video_path, output_folder, frame_rate=1):\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    cap = cv2.VideoCapture(NYC_video)\n",
        "    count = 0\n",
        "    success = True\n",
        "\n",
        "    while success:\n",
        "        success, frame = cap.read()\n",
        "        if count % frame_rate == 0 and success:\n",
        "            frame_filename = os.path.join(output_folder, f\"frame_{count}.jpg\")\n",
        "            cv2.imwrite(frame_filename, frame)\n",
        "            print(f\"Extracted {frame_filename}\")\n",
        "        count += 1\n",
        "\n",
        "    cap.release()\n",
        "\n",
        "extract_frames('NYC_video', 'extracted_frames', frame_rate=30)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-grV1dTYJACx",
        "outputId": "63e345d2-bdee-4926-eeb5-6fd677e7f0aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "0: 384x640 7 persons, 5 cars, 52.5ms\n",
            "Speed: 4.4ms preprocess, 52.5ms inference, 617.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4680.jpg\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7530.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 umbrella, 10.3ms\n",
            "Speed: 2.5ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8220.jpg\n",
            "\n",
            "0: 384x640 5 cars, 7.0ms\n",
            "Speed: 2.6ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2580.jpg\n",
            "\n",
            "0: 384x640 5 cars, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2070.jpg\n",
            "\n",
            "0: 384x640 2 cars, 1 bus, 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1920.jpg\n",
            "\n",
            "0: 384x640 6 cars, 2 buss, 7.6ms\n",
            "Speed: 2.2ms preprocess, 7.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2310.jpg\n",
            "\n",
            "0: 384x640 3 persons, 5 cars, 6.7ms\n",
            "Speed: 2.5ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4620.jpg\n",
            "\n",
            "0: 384x640 4 persons, 3 cars, 6.7ms\n",
            "Speed: 2.6ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4800.jpg\n",
            "\n",
            "0: 384x640 6 persons, 2 cars, 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5130.jpg\n",
            "\n",
            "0: 384x640 1 person, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7080.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 1 truck, 7.0ms\n",
            "Speed: 2.7ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6870.jpg\n",
            "\n",
            "0: 384x640 8 persons, 7.0ms\n",
            "Speed: 3.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8640.jpg\n",
            "\n",
            "0: 384x640 6 cars, 6.7ms\n",
            "Speed: 3.0ms preprocess, 6.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2430.jpg\n",
            "\n",
            "0: 384x640 2 cars, 6.9ms\n",
            "Speed: 2.9ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1200.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 1 umbrella, 7.1ms\n",
            "Speed: 3.0ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7320.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3300.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 7.0ms\n",
            "Speed: 3.1ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3600.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 6.7ms\n",
            "Speed: 3.0ms preprocess, 6.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7680.jpg\n",
            "\n",
            "0: 384x640 4 cars, 9.6ms\n",
            "Speed: 2.2ms preprocess, 9.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1650.jpg\n",
            "\n",
            "0: 384x640 7 persons, 4 cars, 10.2ms\n",
            "Speed: 3.2ms preprocess, 10.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5640.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1890.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 9.3ms\n",
            "Speed: 2.5ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1680.jpg\n",
            "\n",
            "0: 384x640 2 persons, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6990.jpg\n",
            "\n",
            "0: 384x640 7 cars, 10.6ms\n",
            "Speed: 2.1ms preprocess, 10.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1260.jpg\n",
            "\n",
            "0: 384x640 4 cars, 9.0ms\n",
            "Speed: 3.3ms preprocess, 9.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1230.jpg\n",
            "\n",
            "0: 384x640 2 persons, 2 cars, 9.3ms\n",
            "Speed: 3.3ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7620.jpg\n",
            "\n",
            "0: 384x640 9 persons, 1 car, 1 bus, 10.4ms\n",
            "Speed: 2.8ms preprocess, 10.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7920.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 1 umbrella, 9.7ms\n",
            "Speed: 2.0ms preprocess, 9.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7710.jpg\n",
            "\n",
            "0: 384x640 8 persons, 1 umbrella, 10.4ms\n",
            "Speed: 2.2ms preprocess, 10.4ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8700.jpg\n",
            "\n",
            "0: 384x640 11 persons, 2 cars, 9.6ms\n",
            "Speed: 2.1ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5400.jpg\n",
            "\n",
            "0: 384x640 6 cars, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1560.jpg\n",
            "\n",
            "0: 384x640 2 persons, 2 cars, 9.3ms\n",
            "Speed: 2.6ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6060.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 9.7ms\n",
            "Speed: 1.8ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5010.jpg\n",
            "\n",
            "0: 384x640 6 persons, 1 car, 10.1ms\n",
            "Speed: 2.0ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8610.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 14.4ms\n",
            "Speed: 5.1ms preprocess, 14.4ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3510.jpg\n",
            "\n",
            "0: 384x640 5 cars, 9.4ms\n",
            "Speed: 2.6ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1410.jpg\n",
            "\n",
            "0: 384x640 4 persons, 9.3ms\n",
            "Speed: 1.7ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6330.jpg\n",
            "\n",
            "0: 384x640 6 persons, 6.4ms\n",
            "Speed: 1.8ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6810.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 6.4ms\n",
            "Speed: 1.9ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3570.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 6.7ms\n",
            "Speed: 2.7ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3960.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6720.jpg\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7230.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 6.6ms\n",
            "Speed: 2.5ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1860.jpg\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 7.4ms\n",
            "Speed: 1.8ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_720.jpg\n",
            "\n",
            "0: 384x640 1 car, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2790.jpg\n",
            "\n",
            "0: 384x640 1 car, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2850.jpg\n",
            "\n",
            "0: 384x640 2 cars, 6.9ms\n",
            "Speed: 2.5ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1950.jpg\n",
            "\n",
            "0: 384x640 4 persons, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7290.jpg\n",
            "\n",
            "0: 384x640 5 persons, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8520.jpg\n",
            "\n",
            "0: 384x640 6 persons, 4 cars, 6.7ms\n",
            "Speed: 1.8ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5790.jpg\n",
            "\n",
            "0: 384x640 6 persons, 1 car, 1 bus, 1 truck, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7950.jpg\n",
            "\n",
            "0: 384x640 6 persons, 2 cars, 9.6ms\n",
            "Speed: 3.8ms preprocess, 9.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5070.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 1 bus, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3720.jpg\n",
            "\n",
            "0: 384x640 1 car, 8.8ms\n",
            "Speed: 3.7ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4380.jpg\n",
            "\n",
            "0: 384x640 2 cars, 9.7ms\n",
            "Speed: 3.0ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2940.jpg\n",
            "\n",
            "0: 384x640 7 persons, 9.0ms\n",
            "Speed: 2.2ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6180.jpg\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 9.9ms\n",
            "Speed: 2.5ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2700.jpg\n",
            "\n",
            "0: 384x640 2 persons, 9.8ms\n",
            "Speed: 1.9ms preprocess, 9.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7110.jpg\n",
            "\n",
            "0: 384x640 10 persons, 5 cars, 9.8ms\n",
            "Speed: 3.2ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5820.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 cars, 9.4ms\n",
            "Speed: 2.1ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3990.jpg\n",
            "\n",
            "0: 384x640 2 persons, 2 cars, 10.0ms\n",
            "Speed: 2.0ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4560.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 9.7ms\n",
            "Speed: 2.2ms preprocess, 9.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6570.jpg\n",
            "\n",
            "0: 384x640 9 persons, 2 cars, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5370.jpg\n",
            "\n",
            "0: 384x640 3 cars, 9.4ms\n",
            "Speed: 1.7ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_510.jpg\n",
            "\n",
            "0: 384x640 5 cars, 9.1ms\n",
            "Speed: 2.0ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_840.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 6.6ms\n",
            "Speed: 2.4ms preprocess, 6.6ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2100.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 6.5ms\n",
            "Speed: 1.9ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3900.jpg\n",
            "\n",
            "0: 384x640 5 persons, 6.8ms\n",
            "Speed: 2.1ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8460.jpg\n",
            "\n",
            "0: 384x640 3 persons, 12.0ms\n",
            "Speed: 2.3ms preprocess, 12.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6390.jpg\n",
            "\n",
            "0: 384x640 9 persons, 1 umbrella, 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8340.jpg\n",
            "\n",
            "0: 384x640 1 person, 5 cars, 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2190.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 7.4ms\n",
            "Speed: 2.0ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2460.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2610.jpg\n",
            "\n",
            "0: 384x640 2 cars, 1 bus, 6.4ms\n",
            "Speed: 2.9ms preprocess, 6.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1770.jpg\n",
            "\n",
            "0: 384x640 2 cars, 6.5ms\n",
            "Speed: 2.7ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4320.jpg\n",
            "\n",
            "0: 384x640 9 persons, 1 car, 1 umbrella, 7.8ms\n",
            "Speed: 2.1ms preprocess, 7.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8670.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 6.9ms\n",
            "Speed: 2.6ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6750.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 7.0ms\n",
            "Speed: 2.1ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2730.jpg\n",
            "\n",
            "0: 384x640 6 cars, 6.5ms\n",
            "Speed: 2.5ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1290.jpg\n",
            "\n",
            "0: 384x640 2 cars, 7.0ms\n",
            "Speed: 3.3ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4260.jpg\n",
            "\n",
            "0: 384x640 1 person, 3 cars, 7.1ms\n",
            "Speed: 2.8ms preprocess, 7.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4200.jpg\n",
            "\n",
            "0: 384x640 4 cars, 8.6ms\n",
            "Speed: 4.8ms preprocess, 8.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2040.jpg\n",
            "\n",
            "0: 384x640 2 persons, 2 cars, 7.4ms\n",
            "Speed: 1.9ms preprocess, 7.4ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2970.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 10.7ms\n",
            "Speed: 1.6ms preprocess, 10.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4770.jpg\n",
            "\n",
            "0: 384x640 4 cars, 9.8ms\n",
            "Speed: 1.8ms preprocess, 9.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1980.jpg\n",
            "\n",
            "0: 384x640 5 cars, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_930.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3330.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 umbrella, 9.4ms\n",
            "Speed: 2.0ms preprocess, 9.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8580.jpg\n",
            "\n",
            "0: 384x640 4 cars, 9.1ms\n",
            "Speed: 3.0ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_90.jpg\n",
            "\n",
            "0: 384x640 3 cars, 1 bus, 10.6ms\n",
            "Speed: 2.3ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1710.jpg\n",
            "\n",
            "0: 384x640 1 car, 11.1ms\n",
            "Speed: 2.5ms preprocess, 11.1ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3360.jpg\n",
            "\n",
            "0: 384x640 7 cars, 11.3ms\n",
            "Speed: 3.1ms preprocess, 11.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_210.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 10.3ms\n",
            "Speed: 2.8ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_150.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 9.6ms\n",
            "Speed: 2.6ms preprocess, 9.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6120.jpg\n",
            "\n",
            "0: 384x640 8 cars, 9.6ms\n",
            "Speed: 1.9ms preprocess, 9.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_990.jpg\n",
            "\n",
            "0: 384x640 7 persons, 4 cars, 10.4ms\n",
            "Speed: 1.8ms preprocess, 10.4ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5670.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 9.5ms\n",
            "Speed: 2.3ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6900.jpg\n",
            "\n",
            "0: 384x640 4 persons, 9.3ms\n",
            "Speed: 1.6ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7380.jpg\n",
            "\n",
            "0: 384x640 4 persons, 2 cars, 9.0ms\n",
            "Speed: 2.5ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4650.jpg\n",
            "\n",
            "0: 384x640 1 person, 3 cars, 9.0ms\n",
            "Speed: 2.8ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5340.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 9.7ms\n",
            "Speed: 2.1ms preprocess, 9.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6150.jpg\n",
            "\n",
            "0: 384x640 5 persons, 14.6ms\n",
            "Speed: 3.0ms preprocess, 14.6ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6240.jpg\n",
            "\n",
            "0: 384x640 1 person, 3 cars, 7.0ms\n",
            "Speed: 2.2ms preprocess, 7.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4590.jpg\n",
            "\n",
            "0: 384x640 3 persons, 6.7ms\n",
            "Speed: 2.3ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7980.jpg\n",
            "\n",
            "0: 384x640 6 persons, 6.7ms\n",
            "Speed: 1.8ms preprocess, 6.7ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5850.jpg\n",
            "\n",
            "0: 384x640 2 persons, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6300.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 horse, 1 umbrella, 6.8ms\n",
            "Speed: 2.8ms preprocess, 6.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5940.jpg\n",
            "\n",
            "0: 384x640 3 persons, 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7260.jpg\n",
            "\n",
            "0: 384x640 5 cars, 7.1ms\n",
            "Speed: 3.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2130.jpg\n",
            "\n",
            "0: 384x640 5 cars, 7.2ms\n",
            "Speed: 2.7ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2160.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 6.8ms\n",
            "Speed: 3.0ms preprocess, 6.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3450.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 umbrella, 6.9ms\n",
            "Speed: 2.8ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8070.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 9.0ms\n",
            "Speed: 3.9ms preprocess, 9.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6090.jpg\n",
            "\n",
            "0: 384x640 2 persons, 7.1ms\n",
            "Speed: 3.2ms preprocess, 7.1ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7440.jpg\n",
            "\n",
            "0: 384x640 6 persons, 2 cars, 6.9ms\n",
            "Speed: 2.8ms preprocess, 6.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5280.jpg\n",
            "\n",
            "0: 384x640 6 cars, 9.3ms\n",
            "Speed: 2.1ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_450.jpg\n",
            "\n",
            "0: 384x640 7 cars, 10.1ms\n",
            "Speed: 3.7ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1530.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 car, 1 bus, 1 truck, 10.2ms\n",
            "Speed: 1.9ms preprocess, 10.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7860.jpg\n",
            "\n",
            "0: 384x640 3 persons, 10.1ms\n",
            "Speed: 4.4ms preprocess, 10.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7050.jpg\n",
            "\n",
            "0: 384x640 3 persons, 10.7ms\n",
            "Speed: 2.6ms preprocess, 10.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6450.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 9.2ms\n",
            "Speed: 2.4ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2640.jpg\n",
            "\n",
            "0: 384x640 5 persons, 9.2ms\n",
            "Speed: 2.0ms preprocess, 9.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6690.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 9.0ms\n",
            "Speed: 1.8ms preprocess, 9.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8130.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 umbrellas, 9.3ms\n",
            "Speed: 3.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8160.jpg\n",
            "\n",
            "0: 384x640 1 person, 4 cars, 8.9ms\n",
            "Speed: 2.4ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5250.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 9.2ms\n",
            "Speed: 3.3ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7500.jpg\n",
            "\n",
            "0: 384x640 1 car, 1 umbrella, 9.2ms\n",
            "Speed: 2.6ms preprocess, 9.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7740.jpg\n",
            "\n",
            "0: 384x640 8 persons, 2 cars, 10.2ms\n",
            "Speed: 2.9ms preprocess, 10.2ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5040.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 11.7ms\n",
            "Speed: 1.9ms preprocess, 11.7ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_0.jpg\n",
            "\n",
            "0: 384x640 2 persons, 4 cars, 1 bus, 11.0ms\n",
            "Speed: 3.0ms preprocess, 11.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2400.jpg\n",
            "\n",
            "0: 384x640 2 persons, 11.6ms\n",
            "Speed: 2.9ms preprocess, 11.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6600.jpg\n",
            "\n",
            "0: 384x640 3 cars, 14.1ms\n",
            "Speed: 1.9ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_570.jpg\n",
            "\n",
            "0: 384x640 1 person, 8.6ms\n",
            "Speed: 2.3ms preprocess, 8.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8040.jpg\n",
            "\n",
            "0: 384x640 4 persons, 2 cars, 11.9ms\n",
            "Speed: 2.4ms preprocess, 11.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4920.jpg\n",
            "\n",
            "0: 384x640 5 cars, 15.4ms\n",
            "Speed: 1.9ms preprocess, 15.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_660.jpg\n",
            "\n",
            "0: 384x640 6 cars, 8.2ms\n",
            "Speed: 2.4ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_270.jpg\n",
            "\n",
            "0: 384x640 6 cars, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_390.jpg\n",
            "\n",
            "0: 384x640 8 cars, 1 bus, 8.3ms\n",
            "Speed: 2.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_180.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 car, 1 umbrella, 8.5ms\n",
            "Speed: 2.0ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8550.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 8.8ms\n",
            "Speed: 1.9ms preprocess, 8.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2520.jpg\n",
            "\n",
            "0: 384x640 5 cars, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1320.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 8.0ms\n",
            "Speed: 1.9ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3390.jpg\n",
            "\n",
            "0: 384x640 7 persons, 3 cars, 8.9ms\n",
            "Speed: 1.9ms preprocess, 8.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5760.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 12.6ms\n",
            "Speed: 1.9ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8370.jpg\n",
            "\n",
            "0: 384x640 4 cars, 9.0ms\n",
            "Speed: 2.0ms preprocess, 9.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1380.jpg\n",
            "\n",
            "0: 384x640 2 persons, 3 cars, 10.9ms\n",
            "Speed: 2.4ms preprocess, 10.9ms inference, 6.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4050.jpg\n",
            "\n",
            "0: 384x640 7 cars, 10.7ms\n",
            "Speed: 3.1ms preprocess, 10.7ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_420.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 14.4ms\n",
            "Speed: 3.3ms preprocess, 14.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3150.jpg\n",
            "\n",
            "0: 384x640 6 cars, 11.0ms\n",
            "Speed: 4.0ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_870.jpg\n",
            "\n",
            "0: 384x640 5 cars, 11.4ms\n",
            "Speed: 2.5ms preprocess, 11.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_360.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 9.5ms\n",
            "Speed: 2.3ms preprocess, 9.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_480.jpg\n",
            "\n",
            "0: 384x640 1 person, 8 cars, 1 bus, 13.0ms\n",
            "Speed: 1.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_690.jpg\n",
            "\n",
            "0: 384x640 2 persons, 10.2ms\n",
            "Speed: 2.1ms preprocess, 10.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3750.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 cars, 9.0ms\n",
            "Speed: 2.3ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5220.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 10.3ms\n",
            "Speed: 2.3ms preprocess, 10.3ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4530.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 12.6ms\n",
            "Speed: 2.5ms preprocess, 12.6ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3660.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 12.9ms\n",
            "Speed: 2.3ms preprocess, 12.9ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6960.jpg\n",
            "\n",
            "0: 384x640 4 persons, 14.9ms\n",
            "Speed: 2.0ms preprocess, 14.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7410.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 11.5ms\n",
            "Speed: 2.2ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_900.jpg\n",
            "\n",
            "0: 384x640 2 persons, 3 cars, 9.3ms\n",
            "Speed: 2.0ms preprocess, 9.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4080.jpg\n",
            "\n",
            "0: 384x640 8 persons, 1 car, 13.1ms\n",
            "Speed: 1.9ms preprocess, 13.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8310.jpg\n",
            "\n",
            "0: 384x640 1 car, 1 bus, 9.3ms\n",
            "Speed: 2.2ms preprocess, 9.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_600.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 1 bus, 11.7ms\n",
            "Speed: 2.3ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5490.jpg\n",
            "\n",
            "0: 384x640 8 persons, 3 cars, 11.7ms\n",
            "Speed: 3.4ms preprocess, 11.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5430.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 8.5ms\n",
            "Speed: 2.0ms preprocess, 8.5ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2280.jpg\n",
            "\n",
            "0: 384x640 3 cars, 10.5ms\n",
            "Speed: 2.1ms preprocess, 10.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1590.jpg\n",
            "\n",
            "0: 384x640 6 persons, 13.2ms\n",
            "Speed: 2.2ms preprocess, 13.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6780.jpg\n",
            "\n",
            "0: 384x640 7 cars, 1 bus, 12.1ms\n",
            "Speed: 2.6ms preprocess, 12.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_330.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 12.8ms\n",
            "Speed: 2.0ms preprocess, 12.8ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8730.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 12.3ms\n",
            "Speed: 2.1ms preprocess, 12.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8010.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 8.4ms\n",
            "Speed: 1.9ms preprocess, 8.4ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4890.jpg\n",
            "\n",
            "0: 384x640 5 cars, 12.0ms\n",
            "Speed: 2.2ms preprocess, 12.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_750.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 11.2ms\n",
            "Speed: 1.9ms preprocess, 11.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3690.jpg\n",
            "\n",
            "0: 384x640 2 cars, 12.8ms\n",
            "Speed: 2.5ms preprocess, 12.8ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4230.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 umbrella, 13.1ms\n",
            "Speed: 1.9ms preprocess, 13.1ms inference, 2.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8490.jpg\n",
            "\n",
            "0: 384x640 6 persons, 10.3ms\n",
            "Speed: 2.0ms preprocess, 10.3ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6360.jpg\n",
            "\n",
            "0: 384x640 5 cars, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 5.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1470.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 11.8ms\n",
            "Speed: 2.0ms preprocess, 11.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7020.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 13.5ms\n",
            "Speed: 2.3ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4020.jpg\n",
            "\n",
            "0: 384x640 2 persons, 2 cars, 11.8ms\n",
            "Speed: 2.8ms preprocess, 11.8ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3000.jpg\n",
            "\n",
            "0: 384x640 1 person, 3 cars, 11.2ms\n",
            "Speed: 2.0ms preprocess, 11.2ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4110.jpg\n",
            "\n",
            "0: 384x640 4 persons, 3 cars, 10.1ms\n",
            "Speed: 1.9ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5580.jpg\n",
            "\n",
            "0: 384x640 1 person, 4 cars, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4170.jpg\n",
            "\n",
            "0: 384x640 5 persons, 8.4ms\n",
            "Speed: 1.8ms preprocess, 8.4ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6510.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 7.6ms\n",
            "Speed: 1.9ms preprocess, 7.6ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8190.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 10.3ms\n",
            "Speed: 2.2ms preprocess, 10.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2490.jpg\n",
            "\n",
            "0: 384x640 7 persons, 2 cars, 8.8ms\n",
            "Speed: 2.0ms preprocess, 8.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5550.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 tv, 9.0ms\n",
            "Speed: 3.2ms preprocess, 9.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1080.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 car, 7.9ms\n",
            "Speed: 2.0ms preprocess, 7.9ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4980.jpg\n",
            "\n",
            "0: 384x640 2 cars, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4290.jpg\n",
            "\n",
            "0: 384x640 8 persons, 1 umbrella, 9.1ms\n",
            "Speed: 2.2ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8430.jpg\n",
            "\n",
            "0: 384x640 4 persons, 7.9ms\n",
            "Speed: 1.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6210.jpg\n",
            "\n",
            "0: 384x640 7 persons, 1 car, 7.5ms\n",
            "Speed: 1.9ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3930.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 horse, 10.3ms\n",
            "Speed: 2.6ms preprocess, 10.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5880.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 13.8ms\n",
            "Speed: 2.7ms preprocess, 13.8ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_120.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 9.4ms\n",
            "Speed: 2.7ms preprocess, 9.4ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6630.jpg\n",
            "\n",
            "0: 384x640 7 persons, 2 cars, 1 bus, 12.3ms\n",
            "Speed: 2.1ms preprocess, 12.3ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5520.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 1 umbrella, 12.8ms\n",
            "Speed: 2.1ms preprocess, 12.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7140.jpg\n",
            "\n",
            "0: 384x640 7 cars, 11.5ms\n",
            "Speed: 2.0ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_240.jpg\n",
            "\n",
            "0: 384x640 2 persons, 13.5ms\n",
            "Speed: 2.1ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6930.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 11.5ms\n",
            "Speed: 2.2ms preprocess, 11.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7350.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 10.7ms\n",
            "Speed: 1.9ms preprocess, 10.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_60.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 11.1ms\n",
            "Speed: 2.0ms preprocess, 11.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3540.jpg\n",
            "\n",
            "0: 384x640 5 cars, 1 bus, 12.6ms\n",
            "Speed: 1.9ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1800.jpg\n",
            "\n",
            "0: 384x640 3 persons, 10.9ms\n",
            "Speed: 2.3ms preprocess, 10.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6420.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 cars, 12.6ms\n",
            "Speed: 4.6ms preprocess, 12.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4710.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 13.6ms\n",
            "Speed: 1.9ms preprocess, 13.6ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2880.jpg\n",
            "\n",
            "0: 384x640 3 cars, 11.6ms\n",
            "Speed: 3.4ms preprocess, 11.6ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2550.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 13.0ms\n",
            "Speed: 1.9ms preprocess, 13.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5100.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6540.jpg\n",
            "\n",
            "0: 384x640 6 persons, 2 cars, 8.0ms\n",
            "Speed: 2.2ms preprocess, 8.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5610.jpg\n",
            "\n",
            "0: 384x640 7 persons, 4 cars, 1 boat, 9.4ms\n",
            "Speed: 2.2ms preprocess, 9.4ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4740.jpg\n",
            "\n",
            "0: 384x640 6 cars, 1 bus, 13.0ms\n",
            "Speed: 1.9ms preprocess, 13.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2340.jpg\n",
            "\n",
            "0: 384x640 4 cars, 11.5ms\n",
            "Speed: 2.3ms preprocess, 11.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1620.jpg\n",
            "\n",
            "0: 384x640 3 persons, 3 cars, 12.6ms\n",
            "Speed: 2.3ms preprocess, 12.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6660.jpg\n",
            "\n",
            "0: 384x640 7 cars, 15.4ms\n",
            "Speed: 2.2ms preprocess, 15.4ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_300.jpg\n",
            "\n",
            "0: 384x640 6 cars, 3 buss, 14.5ms\n",
            "Speed: 2.8ms preprocess, 14.5ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2250.jpg\n",
            "\n",
            "0: 384x640 5 cars, 11.8ms\n",
            "Speed: 2.0ms preprocess, 11.8ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_30.jpg\n",
            "\n",
            "0: 384x640 7 cars, 11.0ms\n",
            "Speed: 1.9ms preprocess, 11.0ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_810.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 14.3ms\n",
            "Speed: 2.8ms preprocess, 14.3ms inference, 2.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3840.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 19.8ms\n",
            "Speed: 3.0ms preprocess, 19.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3240.jpg\n",
            "\n",
            "0: 384x640 6 persons, 5 cars, 12.5ms\n",
            "Speed: 2.1ms preprocess, 12.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4830.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 umbrella, 16.1ms\n",
            "Speed: 2.1ms preprocess, 16.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7200.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 13.5ms\n",
            "Speed: 2.1ms preprocess, 13.5ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3210.jpg\n",
            "\n",
            "0: 384x640 4 persons, 2 cars, 12.5ms\n",
            "Speed: 2.4ms preprocess, 12.5ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5190.jpg\n",
            "\n",
            "0: 384x640 4 cars, 13.1ms\n",
            "Speed: 2.2ms preprocess, 13.1ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1350.jpg\n",
            "\n",
            "0: 384x640 7 cars, 13.4ms\n",
            "Speed: 2.1ms preprocess, 13.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1020.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 12.7ms\n",
            "Speed: 2.3ms preprocess, 12.7ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2820.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 16.2ms\n",
            "Speed: 2.5ms preprocess, 16.2ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3120.jpg\n",
            "\n",
            "0: 384x640 5 persons, 4 cars, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4860.jpg\n",
            "\n",
            "0: 384x640 2 cars, 10.2ms\n",
            "Speed: 2.4ms preprocess, 10.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4470.jpg\n",
            "\n",
            "0: 384x640 3 cars, 1 bus, 10.1ms\n",
            "Speed: 2.4ms preprocess, 10.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1830.jpg\n",
            "\n",
            "0: 384x640 8 persons, 2 cars, 11.0ms\n",
            "Speed: 2.8ms preprocess, 11.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5700.jpg\n",
            "\n",
            "0: 384x640 8 persons, 4 cars, 10.0ms\n",
            "Speed: 2.6ms preprocess, 10.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8280.jpg\n",
            "\n",
            "0: 384x640 1 person, 4 cars, 1 bus, 10.7ms\n",
            "Speed: 2.5ms preprocess, 10.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2760.jpg\n",
            "\n",
            "0: 384x640 1 person, 3 cars, 9.8ms\n",
            "Speed: 2.6ms preprocess, 9.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4140.jpg\n",
            "\n",
            "0: 384x640 15 persons, 1 umbrella, 10.1ms\n",
            "Speed: 2.1ms preprocess, 10.1ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8400.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 11.7ms\n",
            "Speed: 2.2ms preprocess, 11.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1500.jpg\n",
            "\n",
            "0: 384x640 3 cars, 11.9ms\n",
            "Speed: 2.5ms preprocess, 11.9ms inference, 2.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4350.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 car, 12.2ms\n",
            "Speed: 2.4ms preprocess, 12.2ms inference, 2.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6030.jpg\n",
            "\n",
            "0: 384x640 2 persons, 18.0ms\n",
            "Speed: 2.0ms preprocess, 18.0ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6270.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 13.6ms\n",
            "Speed: 3.8ms preprocess, 13.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3270.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 16.2ms\n",
            "Speed: 2.3ms preprocess, 16.2ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4500.jpg\n",
            "\n",
            "0: 384x640 5 cars, 17.5ms\n",
            "Speed: 2.5ms preprocess, 17.5ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_540.jpg\n",
            "\n",
            "0: 384x640 3 cars, 18.1ms\n",
            "Speed: 4.5ms preprocess, 18.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2010.jpg\n",
            "\n",
            "0: 384x640 3 persons, 15.7ms\n",
            "Speed: 2.3ms preprocess, 15.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7470.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 17.0ms\n",
            "Speed: 2.0ms preprocess, 17.0ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3780.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 14.2ms\n",
            "Speed: 3.3ms preprocess, 14.2ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1440.jpg\n",
            "\n",
            "0: 384x640 1 car, 1 umbrella, 13.2ms\n",
            "Speed: 2.4ms preprocess, 13.2ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7560.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 14.1ms\n",
            "Speed: 4.5ms preprocess, 14.1ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2910.jpg\n",
            "\n",
            "0: 384x640 8 cars, 13.2ms\n",
            "Speed: 2.2ms preprocess, 13.2ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2220.jpg\n",
            "\n",
            "0: 384x640 6 cars, 13.5ms\n",
            "Speed: 1.8ms preprocess, 13.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_780.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 15.6ms\n",
            "Speed: 2.0ms preprocess, 15.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3090.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 umbrella, 10.0ms\n",
            "Speed: 2.7ms preprocess, 10.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8100.jpg\n",
            "\n",
            "0: 384x640 8 persons, 3 cars, 11.7ms\n",
            "Speed: 1.9ms preprocess, 11.7ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5730.jpg\n",
            "\n",
            "0: 384x640 5 cars, 12.2ms\n",
            "Speed: 1.9ms preprocess, 12.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2670.jpg\n",
            "\n",
            "0: 384x640 3 persons, 3 cars, 9.1ms\n",
            "Speed: 3.3ms preprocess, 9.1ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5160.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 8.9ms\n",
            "Speed: 2.1ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_630.jpg\n",
            "\n",
            "0: 384x640 1 person, 1 car, 1 bus, 1 kite, 11.2ms\n",
            "Speed: 2.0ms preprocess, 11.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7770.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 1 bus, 1 kite, 9.3ms\n",
            "Speed: 3.4ms preprocess, 9.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7830.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 7.0ms\n",
            "Speed: 1.9ms preprocess, 7.0ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3060.jpg\n",
            "\n",
            "0: 384x640 2 cars, 7.7ms\n",
            "Speed: 1.9ms preprocess, 7.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3030.jpg\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 0.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7590.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 1 bus, 9.6ms\n",
            "Speed: 1.8ms preprocess, 9.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4440.jpg\n",
            "\n",
            "0: 384x640 5 persons, 6.8ms\n",
            "Speed: 2.2ms preprocess, 6.8ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_8250.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 1 tv, 6.5ms\n",
            "Speed: 2.1ms preprocess, 6.5ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_2370.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 6.7ms\n",
            "Speed: 2.1ms preprocess, 6.7ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3810.jpg\n",
            "\n",
            "0: 384x640 6 cars, 11.8ms\n",
            "Speed: 5.7ms preprocess, 11.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_960.jpg\n",
            "\n",
            "0: 384x640 9 persons, 2 cars, 1 truck, 7.7ms\n",
            "Speed: 2.7ms preprocess, 7.7ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5310.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 cars, 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4950.jpg\n",
            "\n",
            "0: 384x640 6 cars, 9.5ms\n",
            "Speed: 3.9ms preprocess, 9.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1050.jpg\n",
            "\n",
            "0: 384x640 2 cars, 11.6ms\n",
            "Speed: 2.1ms preprocess, 11.6ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1140.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 umbrella, 11.7ms\n",
            "Speed: 2.2ms preprocess, 11.7ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6000.jpg\n",
            "\n",
            "0: 384x640 5 persons, 2 cars, 1 bus, 1 kite, 10.1ms\n",
            "Speed: 2.0ms preprocess, 10.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7800.jpg\n",
            "\n",
            "0: 384x640 9 persons, 1 car, 1 truck, 10.0ms\n",
            "Speed: 2.3ms preprocess, 10.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7890.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 car, 10.8ms\n",
            "Speed: 2.2ms preprocess, 10.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3870.jpg\n",
            "\n",
            "0: 384x640 1 car, 9.5ms\n",
            "Speed: 2.6ms preprocess, 9.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3480.jpg\n",
            "\n",
            "0: 384x640 3 cars, 9.8ms\n",
            "Speed: 3.0ms preprocess, 9.8ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1110.jpg\n",
            "\n",
            "0: 384x640 4 persons, 1 horse, 1 umbrella, 9.4ms\n",
            "Speed: 2.2ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5970.jpg\n",
            "\n",
            "0: 384x640 1 person, 2 cars, 7.0ms\n",
            "Speed: 2.4ms preprocess, 7.0ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_4410.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 7.8ms\n",
            "Speed: 2.2ms preprocess, 7.8ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3630.jpg\n",
            "\n",
            "0: 384x640 4 persons, 2 cars, 6.9ms\n",
            "Speed: 2.4ms preprocess, 6.9ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5460.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 7.0ms\n",
            "Speed: 2.0ms preprocess, 7.0ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3420.jpg\n",
            "\n",
            "0: 384x640 2 persons, 1 car, 8.2ms\n",
            "Speed: 2.1ms preprocess, 8.2ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_3180.jpg\n",
            "\n",
            "0: 384x640 5 persons, 1 horse, 7.2ms\n",
            "Speed: 2.0ms preprocess, 7.2ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_5910.jpg\n",
            "\n",
            "0: 384x640 5 persons, 7.6ms\n",
            "Speed: 2.6ms preprocess, 7.6ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6480.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 umbrella, 9.9ms\n",
            "Speed: 2.0ms preprocess, 9.9ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7170.jpg\n",
            "\n",
            "0: 384x640 3 persons, 2 cars, 7.1ms\n",
            "Speed: 2.3ms preprocess, 7.1ms inference, 1.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_6840.jpg\n",
            "\n",
            "0: 384x640 4 cars, 1 bus, 8.3ms\n",
            "Speed: 1.9ms preprocess, 8.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1740.jpg\n",
            "\n",
            "0: 384x640 1 car, 9.4ms\n",
            "Speed: 5.2ms preprocess, 9.4ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_1170.jpg\n",
            "\n",
            "0: 384x640 3 persons, 1 car, 1 umbrella, 7.3ms\n",
            "Speed: 2.3ms preprocess, 7.3ms inference, 1.3ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Generated labels for frame_7650.jpg\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "\n",
        "# Load a pre-trained YOLOv8 model (you can use a smaller model like 'yolov8n.pt')\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "def generate_labels_for_images(image_folder, output_folder):\n",
        "    # Ensure the output folder exists\n",
        "    if not os.path.exists(output_folder):\n",
        "        os.makedirs(output_folder)\n",
        "\n",
        "    # Loop through all images in the folder\n",
        "    for image_filename in os.listdir(image_folder):\n",
        "        if image_filename.endswith(('.jpg', '.png', '.jpeg')):\n",
        "            image_path = os.path.join(image_folder, image_filename)\n",
        "            output_txt_path = os.path.join(output_folder, image_filename.replace('.jpg', '.txt'))\n",
        "\n",
        "            # Read the image\n",
        "            img = cv2.imread(image_path)\n",
        "\n",
        "            # Run inference on the image using the YOLO model\n",
        "            results = model(img)\n",
        "\n",
        "            # Open the output text file to save the annotations\n",
        "            with open(output_txt_path, 'w') as f:\n",
        "                for result in results:\n",
        "                    for box in result.boxes:\n",
        "                        x1, y1, x2, y2 = box.xyxy[0]\n",
        "                        conf = box.conf[0]\n",
        "                        cls = int(box.cls[0])\n",
        "\n",
        "                        # YOLO expects normalized values (between 0 and 1), so normalize bounding box coordinates\n",
        "                        img_height, img_width = img.shape[:2]\n",
        "                        center_x = (x1 + x2) / 2 / img_width\n",
        "                        center_y = (y1 + y2) / 2 / img_height\n",
        "                        width = (x2 - x1) / img_width\n",
        "                        height = (y2 - y1) / img_height\n",
        "\n",
        "                        # Write the annotations in YOLO format: class_id, center_x, center_y, width, height\n",
        "                        f.write(f\"{cls} {center_x} {center_y} {width} {height}\\n\")\n",
        "\n",
        "            print(f\"Generated labels for {image_filename}\")\n",
        "\n",
        "# Example usage:\n",
        "image_folder = \"extracted_frames\"  # Replace with the actual path to your images folder\n",
        "output_folder = \"labels\"  # Replace with the actual path to save the generated labels\n",
        "\n",
        "generate_labels_for_images(image_folder, output_folder)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vr57_vT0NmH8",
        "outputId": "cfee4da9-86ba-4e49-8f6f-5a3c9d4edbb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset has been successfully split into training and validation sets.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import random\n",
        "import shutil\n",
        "\n",
        "# Paths to your images and labels - **UPDATE THESE PATHS**\n",
        "images_folder = \"extracted_frames\"   # Folder where your images are stored\n",
        "labels_folder = \"labels\"   # Folder where the .txt label files are stored\n",
        "output_dataset_folder = \"dataset\"  # Where to create the dataset\n",
        "\n",
        "# Create the necessary folder structure\n",
        "train_images_folder = os.path.join(output_dataset_folder, \"images\", \"train\")\n",
        "valid_images_folder = os.path.join(output_dataset_folder, \"images\", \"valid\")\n",
        "train_labels_folder = os.path.join(output_dataset_folder, \"labels\", \"train\")\n",
        "valid_labels_folder = os.path.join(output_dataset_folder, \"labels\", \"valid\")\n",
        "\n",
        "# Create folders if they don't exist\n",
        "os.makedirs(train_images_folder, exist_ok=True)\n",
        "os.makedirs(valid_images_folder, exist_ok=True)\n",
        "os.makedirs(train_labels_folder, exist_ok=True)\n",
        "os.makedirs(valid_labels_folder, exist_ok=True)\n",
        "\n",
        "# Get the list of all images\n",
        "all_images = [f for f in os.listdir(images_folder) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
        "\n",
        "# Shuffle and split the dataset (80% for training, 20% for validation)\n",
        "random.shuffle(all_images)\n",
        "split_index = int(0.8 * len(all_images))  # 80% of images\n",
        "train_images = all_images[:split_index]\n",
        "valid_images = all_images[split_index:]\n",
        "\n",
        "# Function to move images and labels to the corresponding folders\n",
        "def move_files(image_list, source_images_folder, source_labels_folder, dest_images_folder, dest_labels_folder):\n",
        "    for image_filename in image_list:\n",
        "        image_path = os.path.join(source_images_folder, image_filename)\n",
        "        label_filename = image_filename.replace('.jpg', '.txt').replace('.jpeg', '.txt').replace('.png', '.txt')\n",
        "        label_path = os.path.join(source_labels_folder, label_filename)\n",
        "\n",
        "        # Move the image\n",
        "        shutil.copy(image_path, os.path.join(dest_images_folder, image_filename))\n",
        "\n",
        "        # Move the corresponding label if it exists\n",
        "        if os.path.exists(label_path):\n",
        "            shutil.copy(label_path, os.path.join(dest_labels_folder, label_filename))\n",
        "        else:\n",
        "            print(f\"Warning: Label not found for {image_filename}\")\n",
        "\n",
        "# Move training images and labels\n",
        "move_files(train_images, images_folder, labels_folder, train_images_folder, train_labels_folder)\n",
        "\n",
        "# Move validation images and labels\n",
        "move_files(valid_images, images_folder, labels_folder, valid_images_folder, valid_labels_folder)\n",
        "\n",
        "print(\"Dataset has been successfully split into training and validation sets.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5gJYOsOEHdg",
        "outputId": "d3f0a782-bc81-4224-cfef-1647391c73cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2024-09-05 13:51:15--  https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt\n",
            "Resolving github.com (github.com)... 20.205.243.166\n",
            "Connecting to github.com (github.com)|20.205.243.166|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/732c503e-9fcb-4a82-be7f-106baafbda15?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240905%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240905T135116Z&X-Amz-Expires=300&X-Amz-Signature=721bdcf3d5f7a0483f48a468024b1f99d21c9bad9c9a15c803eb2c236977d2d6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8n.pt&response-content-type=application%2Foctet-stream [following]\n",
            "--2024-09-05 13:51:16--  https://objects.githubusercontent.com/github-production-release-asset-2e65be/521807533/732c503e-9fcb-4a82-be7f-106baafbda15?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=releaseassetproduction%2F20240905%2Fus-east-1%2Fs3%2Faws4_request&X-Amz-Date=20240905T135116Z&X-Amz-Expires=300&X-Amz-Signature=721bdcf3d5f7a0483f48a468024b1f99d21c9bad9c9a15c803eb2c236977d2d6&X-Amz-SignedHeaders=host&actor_id=0&key_id=0&repo_id=521807533&response-content-disposition=attachment%3B%20filename%3Dyolov8n.pt&response-content-type=application%2Foctet-stream\n",
            "Resolving objects.githubusercontent.com (objects.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to objects.githubusercontent.com (objects.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 6534387 (6.2M) [application/octet-stream]\n",
            "Saving to: yolov8n.pt.3\n",
            "\n",
            "yolov8n.pt.3        100%[===================>]   6.23M  --.-KB/s    in 0.02s   \n",
            "\n",
            "2024-09-05 13:51:16 (270 MB/s) - yolov8n.pt.3 saved [6534387/6534387]\n",
            "\n",
            "Ultralytics YOLOv8.2.87  Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0mtask=detect, mode=train, model=yolov8n.pt, data=/content/data.yaml, epochs=50, time=None, patience=100, batch=16, imgsz=640, save=True, save_period=-1, cache=False, device=None, workers=8, project=None, name=train3, exist_ok=False, pretrained=True, optimizer=auto, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs/detect/train3\n",
            "Overriding model.yaml nc=80 with nc=4\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
            "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
            "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
            "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
            "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
            "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
            "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
            "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
            "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
            "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
            " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
            " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
            " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
            " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
            " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
            " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
            " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
            " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
            " 22        [15, 18, 21]  1    752092  ultralytics.nn.modules.head.Detect           [4, [64, 128, 256]]           \n",
            "Model summary: 225 layers, 3,011,628 parameters, 3,011,612 gradients, 8.2 GFLOPs\n",
            "\n",
            "Transferred 319/355 items from pretrained weights\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/detect/train3', view at http://localhost:6006/\n",
            "Freezing layer 'model.22.dfl.conv.weight'\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks with YOLOv8n...\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/dataset/labels/train... 281 images, 1 backgrounds, 88 corrupt: 100%|| 281/281 [00:00<00:00, 2240.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_0.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1080.jpg: ignoring corrupt image/label: Label class 62 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_120.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_150.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1500.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1680.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1710.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1740.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1770.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_180.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1800.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1830.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1860.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_1920.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2100.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2250.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2280.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2310.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2340.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2370.jpg: ignoring corrupt image/label: Label class 62 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2400.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2460.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2520.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2610.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2640.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2700.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2730.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_2760.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_3720.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_4440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_4740.jpg: ignoring corrupt image/label: Label class 8 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_480.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5310.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5520.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5880.jpg: ignoring corrupt image/label: Label class 17 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5910.jpg: ignoring corrupt image/label: Label class 17 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5940.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_5970.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_60.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_600.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6000.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6150.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_630.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6540.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6570.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6630.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6720.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6750.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6870.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_690.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6900.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_6960.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7140.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7170.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_720.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7200.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7320.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7560.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7650.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7680.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7710.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7740.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7770.jpg: ignoring corrupt image/label: Label class 33 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7800.jpg: ignoring corrupt image/label: Label class 33 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7830.jpg: ignoring corrupt image/label: Label class 33 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7860.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7890.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7920.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_7950.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8010.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8070.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8100.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8130.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8160.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8190.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8220.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8370.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8400.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8430.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8490.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8550.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8580.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8670.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_8700.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mWARNING  /content/dataset/images/train/frame_900.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/dataset/labels/train.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/valid... 107 images, 0 backgrounds, 40 corrupt: 100%|| 107/107 [00:00<00:00, 1833.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_0.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1080.jpg: ignoring corrupt image/label: Label class 62 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_120.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1500.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1710.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1800.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1890.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1920.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2610.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2700.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2730.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_330.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_4440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_4740.jpg: ignoring corrupt image/label: Label class 8 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_480.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5310.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5880.jpg: ignoring corrupt image/label: Label class 17 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5940.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6150.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_630.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6570.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6720.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7140.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7170.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7320.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7650.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7680.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7710.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7800.jpg: ignoring corrupt image/label: Label class 33 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7950.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8010.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8070.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8100.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8220.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8340.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8400.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8700.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/dataset/labels/valid.cache\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Plotting labels to runs/detect/train3/labels.jpg... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.00125, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
            "\u001b[34m\u001b[1mTensorBoard: \u001b[0mmodel graph visualization added \n",
            "Image sizes 640 train, 640 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1mruns/detect/train3\u001b[0m\n",
            "Starting training for 50 epochs...\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       1/50      2.34G      1.567      4.442      1.064          7        640: 100%|| 13/13 [00:06<00:00,  1.87it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.39it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326   0.000539       0.02    0.00031   0.000112\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       2/50      2.21G      1.704      3.262     0.9693         19        640: 100%|| 13/13 [00:04<00:00,  2.73it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.90it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326    0.00845      0.323      0.255      0.169\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       3/50      2.25G      1.322      2.219     0.9352          3        640: 100%|| 13/13 [00:03<00:00,  4.21it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.58it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326     0.0192      0.688      0.434      0.264\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       4/50      2.26G      1.343      1.848     0.9624          6        640: 100%|| 13/13 [00:02<00:00,  4.36it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.51it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.986     0.0958      0.523      0.359\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       5/50      2.25G      1.418      1.751     0.9368         17        640: 100%|| 13/13 [00:04<00:00,  2.86it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.81it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.921      0.147      0.624      0.441\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       6/50      2.27G      1.294      1.744     0.9424          4        640: 100%|| 13/13 [00:03<00:00,  3.66it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.898      0.334      0.667      0.459\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       7/50      2.27G       1.33      1.618     0.9584         13        640: 100%|| 13/13 [00:03<00:00,  4.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.85it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.822      0.475      0.713      0.515\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       8/50      2.24G      1.293       1.54     0.9305          6        640: 100%|| 13/13 [00:03<00:00,  3.32it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.12it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.725      0.621      0.724      0.516\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "       9/50      2.27G      1.245       1.43     0.9185         20        640: 100%|| 13/13 [00:03<00:00,  3.51it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.69it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.746      0.638      0.744      0.556\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      10/50      2.25G      1.254      1.529     0.9248         10        640: 100%|| 13/13 [00:02<00:00,  4.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.723      0.726      0.787      0.586\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      11/50      2.25G      1.223      1.391     0.9063         16        640: 100%|| 13/13 [00:03<00:00,  3.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.51it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.699       0.62      0.714      0.537\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      12/50      2.27G      1.224      1.402     0.8929         25        640: 100%|| 13/13 [00:05<00:00,  2.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.719       0.72      0.779      0.569\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      13/50      2.26G      1.175      1.416     0.9042         19        640: 100%|| 13/13 [00:02<00:00,  4.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.99it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.744      0.754        0.8       0.59\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      14/50      2.23G      1.107      1.393     0.8899          3        640: 100%|| 13/13 [00:03<00:00,  4.31it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.16it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.729       0.74      0.797      0.594\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      15/50      2.24G      1.152      1.453     0.9422          2        640: 100%|| 13/13 [00:04<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.50it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.725      0.749      0.799      0.608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      16/50      2.28G       1.19       1.36     0.9057          9        640: 100%|| 13/13 [00:03<00:00,  4.26it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.55it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.723      0.747      0.803      0.608\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      17/50      2.21G      1.107      1.331     0.8892          3        640: 100%|| 13/13 [00:03<00:00,  4.30it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.48it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326       0.74      0.763       0.82      0.618\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      18/50      2.26G      1.044      1.225     0.8994          6        640: 100%|| 13/13 [00:04<00:00,  2.65it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.63it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326       0.71      0.714      0.786      0.591\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      19/50      2.25G      1.086      1.288     0.8741          3        640: 100%|| 13/13 [00:02<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.61it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.733      0.795      0.839      0.631\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      20/50      2.25G      1.045      1.193       0.87          9        640: 100%|| 13/13 [00:02<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.751      0.777      0.826      0.636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      21/50      2.23G      1.037      1.154     0.8791         16        640: 100%|| 13/13 [00:04<00:00,  3.07it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.78it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.709      0.814      0.829      0.638\n",
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      22/50      2.26G      1.007      1.164     0.8826          5        640: 100%|| 13/13 [00:03<00:00,  3.92it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.755      0.753      0.826      0.636\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      23/50      2.25G      1.101      1.174     0.8927          2        640: 100%|| 13/13 [00:03<00:00,  4.28it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.727      0.783      0.845      0.664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      24/50      2.26G      1.089      1.144     0.8873          9        640: 100%|| 13/13 [00:04<00:00,  3.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.742      0.816      0.853      0.654\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      25/50      2.27G      1.099      1.188     0.8747         10        640: 100%|| 13/13 [00:03<00:00,  3.49it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.77it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.726      0.819      0.852      0.669\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      26/50      2.24G      1.108       1.14     0.8785         14        640: 100%|| 13/13 [00:02<00:00,  4.35it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.89it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.724      0.835      0.852      0.657\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      27/50      2.25G       1.08      1.101     0.8742         16        640: 100%|| 13/13 [00:03<00:00,  3.61it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.20it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.792      0.763      0.854      0.646\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      28/50      2.22G     0.9512      1.002     0.8691          4        640: 100%|| 13/13 [00:05<00:00,  2.37it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.98it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.782      0.811      0.877      0.686\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      29/50      2.24G      1.042      1.079      0.879          4        640: 100%|| 13/13 [00:03<00:00,  3.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.87it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.806      0.769       0.86      0.674\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      30/50      2.28G      1.037      1.116     0.8716          3        640: 100%|| 13/13 [00:02<00:00,  4.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.752      0.849      0.868      0.697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      31/50      2.24G      1.013      1.237     0.8446          2        640: 100%|| 13/13 [00:04<00:00,  2.85it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.07it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.742      0.836      0.867       0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      32/50      2.24G     0.9781       1.03     0.8913          3        640: 100%|| 13/13 [00:03<00:00,  3.88it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  4.47it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.745       0.82      0.869      0.687\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      33/50      2.24G      1.002      1.175     0.8762          1        640: 100%|| 13/13 [00:02<00:00,  4.45it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  4.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.798      0.811      0.877      0.694\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      34/50      2.25G     0.9687     0.9551     0.8497         14        640: 100%|| 13/13 [00:03<00:00,  3.55it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.35it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.757      0.853      0.875       0.69\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      35/50      2.28G     0.9233     0.9847     0.8584         10        640: 100%|| 13/13 [00:04<00:00,  3.06it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.71it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.769      0.821      0.868      0.697\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      36/50      2.24G      1.038      1.039     0.8836         12        640: 100%|| 13/13 [00:02<00:00,  4.52it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.66it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.791      0.828      0.886      0.704\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      37/50      2.25G     0.9128     0.9552     0.8419          3        640: 100%|| 13/13 [00:02<00:00,  4.34it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.52it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.764      0.858      0.893      0.722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      38/50      2.24G      0.932     0.9662     0.8613          3        640: 100%|| 13/13 [00:04<00:00,  2.64it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.97it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.779       0.86        0.9      0.723\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      39/50      2.21G     0.9524     0.9708     0.8604          2        640: 100%|| 13/13 [00:03<00:00,  4.33it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.27it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.822      0.813      0.901      0.733\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      40/50      2.28G     0.9423     0.9765     0.8535         16        640: 100%|| 13/13 [00:03<00:00,  4.29it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.24it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.881      0.744      0.886      0.708\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Closing dataloader mosaic\n",
            "\u001b[34m\u001b[1malbumentations: \u001b[0mBlur(p=0.01, blur_limit=(3, 7)), MedianBlur(p=0.01, blur_limit=(3, 7)), ToGray(p=0.01), CLAHE(p=0.01, clip_limit=(1, 4.0), tile_grid_size=(8, 8))\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      41/50      2.28G     0.9105      1.021     0.8532          4        640: 100%|| 13/13 [00:08<00:00,  1.48it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  4.68it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.826      0.785      0.892      0.712\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      42/50      2.21G     0.8981      1.015      0.854          4        640: 100%|| 13/13 [00:03<00:00,  4.27it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.93it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.806      0.826      0.895      0.718\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      43/50      2.22G     0.8817     0.9427       0.86          6        640: 100%|| 13/13 [00:03<00:00,  3.54it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.22it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.824      0.837      0.901       0.73\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      44/50      2.25G     0.8717     0.9483     0.8437         10        640: 100%|| 13/13 [00:04<00:00,  2.98it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.64it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326       0.78      0.826      0.896      0.722\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      45/50      2.21G      0.902     0.8804     0.8541          6        640: 100%|| 13/13 [00:02<00:00,  4.60it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.30it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.801      0.786      0.888      0.713\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      46/50      2.24G     0.7998     0.8971     0.8435          4        640: 100%|| 13/13 [00:02<00:00,  4.57it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  2.33it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.811      0.802      0.902      0.731\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      47/50      2.22G     0.8437     0.9022     0.8443          4        640: 100%|| 13/13 [00:04<00:00,  2.78it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.82it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.816      0.837      0.903      0.738\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      48/50      2.21G     0.9183     0.9461     0.8505          7        640: 100%|| 13/13 [00:02<00:00,  4.38it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  4.28it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326       0.82      0.844      0.908      0.727\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      49/50      2.21G     0.9362      0.979     0.8682          6        640: 100%|| 13/13 [00:02<00:00,  4.46it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:00<00:00,  3.80it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326       0.82      0.821      0.904      0.735\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "      50/50      2.22G     0.8004     0.8965     0.8414          5        640: 100%|| 13/13 [00:04<00:00,  2.76it/s]\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:01<00:00,  1.88it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.865      0.802      0.908      0.736\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "50 epochs completed in 0.082 hours.\n",
            "Optimizer stripped from runs/detect/train3/weights/last.pt, 6.2MB\n",
            "Optimizer stripped from runs/detect/train3/weights/best.pt, 6.2MB\n",
            "\n",
            "Validating runs/detect/train3/weights/best.pt...\n",
            "Ultralytics YOLOv8.2.87  Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 3/3 [00:02<00:00,  1.43it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.816      0.837      0.903      0.737\n",
            "                   car         50        181      0.822      0.785      0.888      0.697\n",
            "                   bus         53        145      0.811       0.89      0.917      0.777\n",
            "Speed: 0.1ms preprocess, 4.4ms inference, 0.0ms loss, 2.3ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train3\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "!wget https://github.com/ultralytics/assets/releases/download/v0.0.0/yolov8n.pt # Download the pre-trained weights\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov8n.pt')\n",
        "\n",
        "# Fine-tune the model with your custom data\n",
        "model.train(data='/content/data.yaml', epochs=50, imgsz=640, batch=16)\n",
        "\n",
        "# Save the fine-tuned model weights\n",
        "model.save('yolov8_custom.pt')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rmn-il-CY641",
        "outputId": "9376542f-d234-45e4-fb01-bffbd9e6ab75"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ultralytics YOLOv8.2.87  Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "Model summary (fused): 168 layers, 3,006,428 parameters, 0 gradients, 8.1 GFLOPs\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mScanning /content/dataset/labels/valid.cache... 107 images, 0 backgrounds, 40 corrupt: 100%|| 107/107 [00:00<?, ?it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_0.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1080.jpg: ignoring corrupt image/label: Label class 62 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_120.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1500.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1710.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1800.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1890.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_1920.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2610.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2700.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_2730.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_330.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_4440.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_4740.jpg: ignoring corrupt image/label: Label class 8 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_480.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5310.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5490.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5880.jpg: ignoring corrupt image/label: Label class 17 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_5940.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6150.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_630.jpg: ignoring corrupt image/label: Label class 5 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6570.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_6720.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7140.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7170.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7320.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7650.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7680.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7710.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7800.jpg: ignoring corrupt image/label: Label class 33 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_7950.jpg: ignoring corrupt image/label: Label class 7 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8010.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8070.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8100.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8220.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8340.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8400.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n",
            "\u001b[34m\u001b[1mval: \u001b[0mWARNING  /content/dataset/images/valid/frame_8700.jpg: ignoring corrupt image/label: Label class 25 exceeds dataset class count 4. Possible class labels are 0-3\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|| 5/5 [00:04<00:00,  1.22it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                   all         67        326      0.812      0.837      0.904       0.74\n",
            "                   car         50        181      0.815      0.785      0.886        0.7\n",
            "                   bus         53        145       0.81       0.89      0.922      0.781\n",
            "Speed: 0.3ms preprocess, 20.5ms inference, 0.0ms loss, 6.4ms postprocess per image\n",
            "Results saved to \u001b[1mruns/detect/train32\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "metrics = model.val(data='/content/data.yaml')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XePLWWADcpPp",
        "outputId": "61da8c39-a24f-4025-9c82-bd47e2dd2175"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Precision: 0.8123975968087762\n",
            "Recall: 0.8370927795770623\n"
          ]
        }
      ],
      "source": [
        "# Get the results dictionary from the DetMetrics object\n",
        "results = metrics.results_dict\n",
        "\n",
        "# Extract and print precision and recall\n",
        "precision = results.get('metrics/precision(B)', None)  # Precision value\n",
        "recall = results.get('metrics/recall(B)', None)  # Recall value\n",
        "\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"Recall: {recall}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JMvQxUkmZ_Mg"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "import subprocess\n",
        "import time\n",
        "# Load the fine-tuned YOLO model\n",
        "model = YOLO('yolov8_custom.pt')  # Use your fine-tuned YOLO model\n",
        "\n",
        "# Function to get the stream URL using Streamlink\n",
        "def get_stream_url(page_url):\n",
        "    \"\"\"\n",
        "    Extracts the stream URL using Streamlink for the given page URL.\n",
        "    Returns the best stream URL or None if extraction fails.\n",
        "    \"\"\"\n",
        "    command = f\"streamlink {page_url} best --stream-url\"\n",
        "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "    if result.returncode != 0:\n",
        "        print(f\"Error getting stream URL: {result.stderr}\")\n",
        "        return None\n",
        "    return result.stdout.strip()\n",
        "\n",
        "# Function to capture an image from the stream URL\n",
        "def capture_image(stream_url, output_path):\n",
        "    \"\"\"\n",
        "    Captures an image from the given stream URL and saves it to the output path.\n",
        "    Returns True if successful, otherwise False.\n",
        "    \"\"\"\n",
        "    cap = cv2.VideoCapture(stream_url)\n",
        "\n",
        "    if not cap.isOpened():\n",
        "        print(\"Error: Could not open video stream.\")\n",
        "        return False\n",
        "\n",
        "    ret, frame = cap.read()\n",
        "\n",
        "    if not ret:\n",
        "        print(\"Error: Could not read frame from video stream.\")\n",
        "        cap.release()\n",
        "        return False\n",
        "\n",
        "    # Save the captured frame as an image\n",
        "    cv2.imwrite(output_path, frame)\n",
        "    cap.release()\n",
        "\n",
        "    print(f\"Image captured and saved as {output_path}\")\n",
        "    return True\n",
        "\n",
        "# Function to process frames and count vehicles using YOLO\n",
        "def process_frame(image_path):\n",
        "    \"\"\"\n",
        "    Processes the captured image with YOLOv8 to detect vehicles.\n",
        "    Draws bounding boxes around detected vehicles and saves the processed image.\n",
        "    Returns the count of detected vehicles.\n",
        "    \"\"\"\n",
        "    frame = cv2.imread(image_path)\n",
        "    if frame is None:\n",
        "        print(f\"Error: Could not load image from {image_path}\")\n",
        "        return 0\n",
        "\n",
        "    results = model(frame)\n",
        "    vehicle_count = 0\n",
        "\n",
        "    # Loop through results and detect vehicles\n",
        "    for result in results:\n",
        "        for box in result.boxes:\n",
        "            x1, y1, x2, y2 = box.xyxy[0]\n",
        "            conf = box.conf[0]\n",
        "            cls = int(box.cls[0])\n",
        "            label = model.names[cls]\n",
        "\n",
        "            if label in ['car', 'truck', 'bus', 'motorcycle']:\n",
        "                vehicle_count += 1\n",
        "                # Draw bounding boxes and labels on the frame\n",
        "                cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "                cv2.putText(frame, f'{label} {conf:.2f}', (int(x1), int(y1) - 10),\n",
        "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "    # Save the processed image\n",
        "    processed_image_path = image_path.replace(\".jpg\", \"_processed.jpg\")\n",
        "    cv2.imwrite(processed_image_path, frame)\n",
        "\n",
        "    print(f\"Processed image saved as {processed_image_path}\")\n",
        "    return vehicle_count\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VBjUKKgSuSmD"
      },
      "outputs": [],
      "source": [
        "# # Function to get the stream URL using Streamlink\n",
        "# def get_stream_url(page_url):\n",
        "#     \"\"\"\n",
        "#     Extracts the stream URL using Streamlink for the given page URL.\n",
        "#     Returns the best stream URL or None if extraction fails.\n",
        "#     \"\"\"\n",
        "#     command = f\"streamlink {page_url} best --stream-url\"\n",
        "#     result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
        "\n",
        "#     if result.returncode != 0:\n",
        "#         print(f\"Error getting stream URL: {result.stderr}\")\n",
        "#         return None\n",
        "#     return result.stdout.strip()\n",
        "\n",
        "# # Function to capture an image from the stream URL\n",
        "# def capture_image(stream_url, output_path):\n",
        "#     \"\"\"\n",
        "#     Captures an image from the given stream URL and saves it to the output path.\n",
        "#     Returns True if successful, otherwise False.\n",
        "#     \"\"\"\n",
        "#     cap = cv2.VideoCapture(stream_url)\n",
        "\n",
        "#     if not cap.isOpened():\n",
        "#         print(\"Error: Could not open video stream.\")\n",
        "#         return False\n",
        "\n",
        "#     ret, frame = cap.read()\n",
        "\n",
        "#     if not ret:\n",
        "#         print(\"Error: Could not read frame from video stream.\")\n",
        "#         cap.release()\n",
        "#         return False\n",
        "\n",
        "#     # Save the captured frame as an image\n",
        "#     cv2.imwrite(output_path, frame)\n",
        "#     cap.release()\n",
        "\n",
        "#     print(f\"Image captured and saved as {output_path}\")\n",
        "#     return True\n",
        "\n",
        "# # Function to process frames and count vehicles using YOLO\n",
        "# def process_frame(image_path):\n",
        "#     \"\"\"\n",
        "#     Processes the captured image with YOLOv8 to detect vehicles.\n",
        "#     Draws bounding boxes around detected vehicles and saves the processed image.\n",
        "#     Returns the count of detected vehicles.\n",
        "#     \"\"\"\n",
        "#     frame = cv2.imread(image_path)\n",
        "#     if frame is None:\n",
        "#         print(f\"Error: Could not load image from {image_path}\")\n",
        "#         return 0\n",
        "\n",
        "#     results = model(frame)\n",
        "#     vehicle_count = 0\n",
        "\n",
        "#     # Loop through results and detect vehicles\n",
        "#     for result in results:\n",
        "#         for box in result.boxes:\n",
        "#             x1, y1, x2, y2 = box.xyxy[0]\n",
        "#             conf = box.conf[0]\n",
        "#             cls = int(box.cls[0])\n",
        "#             label = model.names[cls]\n",
        "\n",
        "#             if label in ['car', 'truck', 'bus', 'motorcycle']:\n",
        "#                 vehicle_count += 1\n",
        "#                 # Draw bounding boxes and labels on the frame\n",
        "#                 cv2.rectangle(frame, (int(x1), int(y1)), (int(x2), int(y2)), (255, 0, 0), 2)\n",
        "#                 cv2.putText(frame, f'{label} {conf:.2f}', (int(x1), int(y1) - 10),\n",
        "#                             cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
        "\n",
        "#     # Save the processed image\n",
        "#     processed_image_path = image_path.replace(\".jpg\", \"_processed.jpg\")\n",
        "#     cv2.imwrite(processed_image_path, frame)\n",
        "\n",
        "#     print(f\"Processed image saved as {processed_image_path}\")\n",
        "#     return vehicle_count\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "9y5cLSIiuhYK"
      },
      "outputs": [],
      "source": [
        "def adjust_green_signal_time(vehicle_count):\n",
        "    \"\"\"\n",
        "    Adjusts the green signal time based on the detected vehicle count.\n",
        "    \"\"\"\n",
        "    base_green_time = 20\n",
        "    vehicle_multiplier = 2\n",
        "\n",
        "    green_time = base_green_time + (vehicle_count * vehicle_multiplier)\n",
        "    return green_time\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "E65cn5juukLY"
      },
      "outputs": [],
      "source": [
        "def update_vehicle_count_file(vehicle_count):\n",
        "    \"\"\"\n",
        "    Updates the vehicle count file to store the latest vehicle count.\n",
        "    \"\"\"\n",
        "    with open(\"vehicle_count.txt\", \"w\") as file:\n",
        "        file.write(str(vehicle_count))\n",
        "    print(\"Vehicle count updated in 'vehicle_count.txt'.\")\n",
        "\n",
        "def read_vehicle_count():\n",
        "    \"\"\"\n",
        "    Reads the vehicle count from the 'vehicle_count.txt' file.\n",
        "    Returns the vehicle count as an integer or 0 if the file doesn't exist.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        with open(\"vehicle_count.txt\", \"r\") as file:\n",
        "            vehicle_count = int(file.read())\n",
        "            if vehicle_count < 0:\n",
        "                raise ValueError(\"Vehicle count cannot be negative.\")\n",
        "            return vehicle_count\n",
        "    except (ValueError, FileNotFoundError) as e:\n",
        "        print(f\"Error reading vehicle count from file: {e}\")\n",
        "        return 0\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "kp6DDnrjukFT"
      },
      "outputs": [],
      "source": [
        "def process_traffic_stream():\n",
        "    \"\"\"\n",
        "    Captures images from a traffic stream, processes them with YOLO,\n",
        "    and adjusts the green signal time dynamically based on vehicle detection.\n",
        "    \"\"\"\n",
        "    page_url = \"https://videos-3.earthcam.com/fecnetwork/15559.flv/chunklist_w1299342573.m3u8\"\n",
        "    stream_url = get_stream_url(page_url)\n",
        "\n",
        "    if not stream_url:\n",
        "        print(\"Error: Could not extract stream URL. Exiting.\")\n",
        "        return\n",
        "\n",
        "    interval_seconds = 5  # Time interval between captures\n",
        "\n",
        "    while True:\n",
        "        timestamp = time.strftime(\"%Y%m%d%H%M%S\")\n",
        "        image_filename = f\"captured_image_{timestamp}.jpg\"\n",
        "\n",
        "        # Capture image from stream\n",
        "        if not capture_image(stream_url, image_filename):\n",
        "            print(\"Stream could not be opened or frame not captured. Exiting.\")\n",
        "            break\n",
        "\n",
        "        # Process the image with YOLO and get the vehicle count\n",
        "        vehicle_count = process_frame(image_filename)\n",
        "        update_vehicle_count_file(vehicle_count)\n",
        "\n",
        "        print(f\"Total vehicles detected: {vehicle_count}\")\n",
        "\n",
        "        # Immediately adjust the green signal based on the current vehicle count\n",
        "        green_time = adjust_green_signal_time(vehicle_count)\n",
        "        print(f\"Adjusted Green Signal Time: {green_time} seconds\")\n",
        "\n",
        "        # Save the adjusted green signal time to a file\n",
        "        with open(\"adjusted_green_time.txt\", \"w\") as output_file:\n",
        "            output_file.write(f\"Green signal adjusted to: {green_time} seconds\")\n",
        "\n",
        "        print(f\"Adjusted green time saved in 'adjusted_green_time.txt'.\")\n",
        "\n",
        "        time.sleep(interval_seconds)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "bFxThiRTwm2J"
      },
      "outputs": [],
      "source": [
        "def adjust_traffic_signal():\n",
        "    \"\"\"\n",
        "    Adjusts the traffic green signal time based on the vehicle count stored in 'vehicle_count.txt'.\n",
        "    \"\"\"\n",
        "    vehicle_count = read_vehicle_count()\n",
        "\n",
        "    # Adjust green signal time based on vehicle count\n",
        "    green_time = adjust_green_signal_time(vehicle_count)\n",
        "    print(f\"Adjusted Green Signal Time: {green_time} seconds\")\n",
        "\n",
        "    # Save the adjusted green time to a file\n",
        "    with open(\"adjusted_green_time.txt\", \"w\") as output_file:\n",
        "        output_file.write(f\"Green signal adjusted to: {green_time} seconds\")\n",
        "    print(f\"Adjusted green time saved in 'adjusted_green_time.txt'.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xY6Yy-1aup1M",
        "outputId": "2a8fb26a-9117-46af-ebea-ba1e71e20cc7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Image captured and saved as captured_image_20240905140242.jpg\n",
            "\n",
            "0: 384x640 1 bus, 10.6ms\n",
            "Speed: 4.3ms preprocess, 10.6ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed image saved as captured_image_20240905140242_processed.jpg\n",
            "Vehicle count updated in 'vehicle_count.txt'.\n",
            "Total vehicles detected: 1\n",
            "Adjusted Green Signal Time: 22 seconds\n",
            "Adjusted green time saved in 'adjusted_green_time.txt'.\n",
            "Image captured and saved as captured_image_20240905140252.jpg\n",
            "\n",
            "0: 384x640 (no detections), 8.9ms\n",
            "Speed: 4.2ms preprocess, 8.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed image saved as captured_image_20240905140252_processed.jpg\n",
            "Vehicle count updated in 'vehicle_count.txt'.\n",
            "Total vehicles detected: 0\n",
            "Adjusted Green Signal Time: 20 seconds\n",
            "Adjusted green time saved in 'adjusted_green_time.txt'.\n",
            "Image captured and saved as captured_image_20240905140302.jpg\n",
            "\n",
            "0: 384x640 1 bus, 8.2ms\n",
            "Speed: 4.8ms preprocess, 8.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Processed image saved as captured_image_20240905140302_processed.jpg\n",
            "Vehicle count updated in 'vehicle_count.txt'.\n",
            "Total vehicles detected: 1\n",
            "Adjusted Green Signal Time: 22 seconds\n",
            "Adjusted green time saved in 'adjusted_green_time.txt'.\n",
            "Error: Could not open video stream.\n",
            "Stream could not be opened or frame not captured. Exiting.\n",
            "Adjusted Green Signal Time: 22 seconds\n",
            "Adjusted green time saved in 'adjusted_green_time.txt'.\n"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    process_traffic_stream()  # Run the traffic stream processing\n",
        "    adjust_traffic_signal()   # Adjust the green signal time based on vehicle count\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
