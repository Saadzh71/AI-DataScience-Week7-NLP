{"cells":[{"cell_type":"markdown","source":["**NOTE! Change run time type to T4 GPU inorder to speed up the process time.**"],"metadata":{"id":"Ig1dTbzPHeIm"}},{"source":["\n","# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES\n","# TO THE CORRECT LOCATION (/kaggle/input) IN YOUR NOTEBOOK,\n","# THEN FEEL FREE TO DELETE THIS CELL.\n","# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n","# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n","# NOTEBOOK.\n","\n","import os\n","import sys\n","from tempfile import NamedTemporaryFile\n","from urllib.request import urlopen\n","from urllib.parse import unquote, urlparse\n","from urllib.error import HTTPError\n","from zipfile import ZipFile\n","import tarfile\n","import shutil\n","\n","CHUNK_SIZE = 40960\n","DATA_SOURCE_MAPPING = 'tweets-blogs-news-swiftkey-dataset-4million:https%3A%2F%2Fstorage.googleapis.com%2Fkaggle-data-sets%2F6261%2F9186%2Fbundle%2Farchive.zip%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com%252F20240901%252Fauto%252Fstorage%252Fgoog4_request%26X-Goog-Date%3D20240901T142205Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D36ffc720428430960a7d0b7a44b0ff2d6011bbb1e559da0fc3ecd1f29b69e4b0c729d6d7195a8ddf46b1db9ffc1a701f670926e9a417b2050b132bd52a6e6355f672ae04512185c27440464778334ddc11bee13e87b540505340bdebcea015471343d86bb6a15f3367ea2a866bffda3cb07879c12bbc9811260dae8c3974868bd24bb687f78c4b8d9c467a901e7450f36a5a44b7d1e474a36cc5b5e291b69cc754955317e472b5a145445892d25e5a83d62cc1d5b0b4b8cb6e9a97f062dd542accc102bde8dfc0336556dae263987cc797752e99f94fa32a7d9320d9f8155c3a0680c6d5310fe3b5f7fdd2ebc195def4ac74e80e6ff1f1389c76d50a7a605e84'\n","\n","KAGGLE_INPUT_PATH='/kaggle/input'\n","KAGGLE_WORKING_PATH='/kaggle/working'\n","KAGGLE_SYMLINK='kaggle'\n","\n","!umount /kaggle/input/ 2> /dev/null\n","shutil.rmtree('/kaggle/input', ignore_errors=True)\n","os.makedirs(KAGGLE_INPUT_PATH, 0o777, exist_ok=True)\n","os.makedirs(KAGGLE_WORKING_PATH, 0o777, exist_ok=True)\n","\n","try:\n","  os.symlink(KAGGLE_INPUT_PATH, os.path.join(\"..\", 'input'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","try:\n","  os.symlink(KAGGLE_WORKING_PATH, os.path.join(\"..\", 'working'), target_is_directory=True)\n","except FileExistsError:\n","  pass\n","\n","for data_source_mapping in DATA_SOURCE_MAPPING.split(','):\n","    directory, download_url_encoded = data_source_mapping.split(':')\n","    download_url = unquote(download_url_encoded)\n","    filename = urlparse(download_url).path\n","    destination_path = os.path.join(KAGGLE_INPUT_PATH, directory)\n","    try:\n","        with urlopen(download_url) as fileres, NamedTemporaryFile() as tfile:\n","            total_length = fileres.headers['content-length']\n","            print(f'Downloading {directory}, {total_length} bytes compressed')\n","            dl = 0\n","            data = fileres.read(CHUNK_SIZE)\n","            while len(data) > 0:\n","                dl += len(data)\n","                tfile.write(data)\n","                done = int(50 * dl / int(total_length))\n","                sys.stdout.write(f\"\\r[{'=' * done}{' ' * (50-done)}] {dl} bytes downloaded\")\n","                sys.stdout.flush()\n","                data = fileres.read(CHUNK_SIZE)\n","            if filename.endswith('.zip'):\n","              with ZipFile(tfile) as zfile:\n","                zfile.extractall(destination_path)\n","            else:\n","              with tarfile.open(tfile.name) as tarfile:\n","                tarfile.extractall(destination_path)\n","            print(f'\\nDownloaded and uncompressed: {directory}')\n","    except HTTPError as e:\n","        print(f'Failed to load (likely expired) {download_url} to path {destination_path}')\n","        continue\n","    except OSError as e:\n","        print(f'Failed to load {download_url} to path {destination_path}')\n","        continue\n","\n","print('Data source import complete.')\n"],"metadata":{"id":"trUtaA-99vU6"},"cell_type":"code","outputs":[],"execution_count":null},{"metadata":{"id":"loV-iP5I9vVB"},"cell_type":"markdown","source":["<a id=\"basic-setup\"></a>\n","# üìÇ Basic Setup"]},{"metadata":{"id":"kYncFEoI9vVD"},"cell_type":"markdown","source":["In this **hidden** code cell, we'll import our packages. As we'll implement n-gram models from scratch we'll just use numpy and Additionally nltk  (just for Tokenization)."]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-input":true,"_kg_hide-output":true,"id":"m7egLAtY9vVD","executionInfo":{"status":"ok","timestamp":1725205660525,"user_tz":-180,"elapsed":4856,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["%%capture\n","\n","## Importing Packages\n","import math\n","import nltk\n","import random\n","import pickle\n","import numpy as np\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","\n","## Basic File Paths\n","data_dir = \"../input/tweets-blogs-news-swiftkey-dataset-4million/final/en_US\"\n","file_path = data_dir + \"/en_US.twitter.txt\"\n","\n","## nltk settings\n","nltk.data.path.append(data_dir)\n","nltk.download('punkt')\n","\n","## Opening the File in read mode (\"r\")\n","with open(file_path, \"r\") as f:\n","    data = f.read()"],"execution_count":2,"outputs":[]},{"metadata":{"id":"tAYi2THj9vVD"},"cell_type":"markdown","source":["<a id=\"pre-process\"></a>\n","# üßΩ Pre-Processing pipeline (Note: As our dataset is quite big, this process will take up to 10 min)"]},{"metadata":{"id":"MB_QcJ8d9vVE"},"cell_type":"markdown","source":["We create a simple pipeline function which:\n","\n","* splits the datasets by the `\\n` character\n","\n","* remove leading and trailing spaces\n","\n","* drop empty sentences.\n","\n","* Tokenize sentences using `nltk.word_tokenize`"]},{"metadata":{"trusted":true,"id":"ADmvu0C09vVF","executionInfo":{"status":"ok","timestamp":1725206064561,"user_tz":-180,"elapsed":399044,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def preprocess_pipeline(data) -> 'list':\n","\n","    # Split by newline character\n","    sentences = data.split('\\n')\n","\n","    # Remove leading and trailing spaces\n","    sentences = [s.strip() for s in sentences]\n","\n","    # Drop Empty Sentences\n","    sentences = [s for s in sentences if len(s) > 0]\n","\n","    # Empty List to hold Tokenized Sentences\n","    tokenized = []\n","\n","    # Iterate through sentences\n","    for sentence in sentences:\n","\n","        # Convert to lowercase\n","        sentence = sentence.lower()\n","\n","        # Convert to a list of words\n","        token = nltk.word_tokenize(sentence)\n","\n","        # Append to list\n","        tokenized.append(token)\n","\n","    return tokenized\n","\n","\n","## Pass our data to this function\n","tokenized_sentences = preprocess_pipeline(data)"],"execution_count":3,"outputs":[]},{"metadata":{"id":"8YxS-dYa9vVH"},"cell_type":"markdown","source":["<a id=\"split\"></a>\n","# ‚úÇÔ∏è Splitting into Train, Valid and Test"]},{"metadata":{"trusted":true,"id":"dhhHIUPQ9vVH","executionInfo":{"status":"ok","timestamp":1725206155431,"user_tz":-180,"elapsed":2840,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["## Obtain Train and Test Split\n","train, test = train_test_split(tokenized_sentences, test_size=0.2, random_state=42)\n","\n","## Obtain Train and Validation Split\n","train, val = train_test_split(train, test_size=0.25, random_state=42)"],"execution_count":4,"outputs":[]},{"metadata":{"id":"ILf3vi7q9vVJ"},"cell_type":"markdown","source":["<a id=\"clean\"></a>\n","# üßπ Cleaning the Data"]},{"metadata":{"id":"d1BhFO2e9vVJ"},"cell_type":"markdown","source":["<a id=\"frequency\"></a>\n","## üìî Creating a Frequency Dictionary"]},{"metadata":{"id":"K-tbNHJ-9vVJ"},"cell_type":"markdown","source":["As our dataset is quite big, we'll only use those words that appear `k` times in our dataset. In this function, we'll create a frequency dictionary for our vocabulary."]},{"metadata":{"trusted":true,"id":"Iz7uT10M9vVK","executionInfo":{"status":"ok","timestamp":1725206158601,"user_tz":-180,"elapsed":542,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def count_the_words(sentences) -> 'dict':\n","\n","  # Creating a Dictionary of counts\n","  word_counts = {}\n","\n","  # Iterating over sentences\n","  for sentence in sentences:\n","\n","    # Iterating over Tokens\n","    for token in sentence:\n","\n","      # Add count for new word\n","      if token not in word_counts.keys():\n","        word_counts[token] = 1\n","\n","      # Increase count by one\n","      else:\n","        word_counts[token] += 1\n","\n","  return word_counts"],"execution_count":5,"outputs":[]},{"metadata":{"id":"ELgmv9We9vVK"},"cell_type":"markdown","source":["<a id=\"closed\"></a>\n","## üîí Creating a Closed Vocabulary"]},{"metadata":{"id":"x8Go1G2V9vVL"},"cell_type":"markdown","source":["One of the most essential steps in dealing with Textual data is handling Out-of-vocabulary words. This helps the model to handle words which are not present in the training corpus. First step in this process is to create a `closed_vocabulary`. This function creates a closed vocabulary containing only those words according to the `count_threshold` parameter."]},{"metadata":{"trusted":true,"id":"bwcY6yxM9vVL","executionInfo":{"status":"ok","timestamp":1725206163216,"user_tz":-180,"elapsed":585,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def handling_oov(tokenized_sentences, count_threshold) -> 'list':\n","\n","  # Empty list for closed vocabulary\n","  closed_vocabulary = []\n","\n","  # Obtain frequency dictionary using previously defined function\n","  words_count = count_the_words(tokenized_sentences)\n","\n","  # Iterate over words and counts\n","  for word, count in words_count.items():\n","\n","    # Append if it's more(or equal) to the threshold\n","    if count >= count_threshold :\n","      closed_vocabulary.append(word)\n","\n","  return closed_vocabulary"],"execution_count":6,"outputs":[]},{"metadata":{"id":"7QaPZ1oc9vVM"},"cell_type":"markdown","source":["<a id=\"unk\"></a>\n","## ü§∑üèª Adding UNK Tokens"]},{"metadata":{"id":"ZZsKw_aB9vVN"},"cell_type":"markdown","source":["In this function we'll add `<unk>` tokens, to those words which are not in the `closed_vocabulary` which we just made."]},{"metadata":{"trusted":true,"id":"7-A1Q0WV9vVN","executionInfo":{"status":"ok","timestamp":1725206166180,"user_tz":-180,"elapsed":538,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def unk_tokenize(tokenized_sentences, vocabulary, unknown_token = \"<unk>\") -> 'list':\n","\n","  # Convert Vocabulary into a set\n","  vocabulary = set(vocabulary)\n","\n","  # Create empty list for sentences\n","  new_tokenized_sentences = []\n","\n","  # Iterate over sentences\n","  for sentence in tokenized_sentences:\n","\n","    # Iterate over sentence and add <unk>\n","    # if the token is absent from the vocabulary\n","    new_sentence = []\n","    for token in sentence:\n","      if token in vocabulary:\n","        new_sentence.append(token)\n","      else:\n","        new_sentence.append(unknown_token)\n","\n","    # Append sentece to the new list\n","    new_tokenized_sentences.append(new_sentence)\n","\n","  return new_tokenized_sentences"],"execution_count":7,"outputs":[]},{"metadata":{"id":"cnA1WqE29vVO"},"cell_type":"markdown","source":["<a id=\"final\"></a>\n","## üßº Final Cleaning Pipeline"]},{"metadata":{"trusted":true,"id":"h5-bh7HV9vVO","executionInfo":{"status":"ok","timestamp":1725206169400,"user_tz":-180,"elapsed":524,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def cleansing(train_data, test_data, count_threshold):\n","\n","  # Get closed Vocabulary\n","  vocabulary = handling_oov(train_data, count_threshold)\n","\n","  # Updated Training Dataset\n","  new_train_data = unk_tokenize(train_data, vocabulary)\n","\n","  # Updated Test Dataset\n","  new_test_data = unk_tokenize(test_data, vocabulary)\n","\n","  return new_train_data, new_test_data, vocabulary"],"execution_count":8,"outputs":[]},{"metadata":{"trusted":true,"id":"90wf2O1E9vVP","executionInfo":{"status":"ok","timestamp":1725206198521,"user_tz":-180,"elapsed":26602,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["min_freq = 6\n","final_train, final_test, vocabulary = cleansing(train, test, min_freq)"],"execution_count":9,"outputs":[]},{"metadata":{"id":"nl2T-LZp9vVQ"},"cell_type":"markdown","source":["<a id=\"build\"></a>\n","# üí™üèª Building The \"Model\""]},{"metadata":{"id":"rcceOwqr9vVQ"},"cell_type":"markdown","source":["This is a helper function, which will come in handy during inference. This function returns a mapping from n-grams to their frequency in the dataset."]},{"metadata":{"trusted":true,"id":"C3EUaAyL9vVQ","executionInfo":{"status":"ok","timestamp":1725206403282,"user_tz":-180,"elapsed":511,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def count_n_grams(data, n, start_token = \"<s>\", end_token = \"<e>\") -> 'dict':\n","\n","  # Empty dict for n-grams\n","  n_grams = {}\n","\n","  # Iterate over all sentences in the dataset\n","  for sentence in data:\n","\n","    # Append n start tokens and a single end token to the sentence\n","    sentence = [start_token]*n + sentence + [end_token]\n","\n","    # Convert the sentence into a tuple\n","    sentence = tuple(sentence)\n","\n","    # Temp var to store length from start of n-gram to end\n","    m = len(sentence) if n==1 else len(sentence)-1\n","\n","    # Iterate over this length\n","    for i in range(m):\n","\n","      # Get the n-gram\n","      n_gram = sentence[i:i+n]\n","\n","      # Add the count of n-gram as value to our dictionary\n","      # IF n-gram is already present\n","      if n_gram in n_grams.keys():\n","        n_grams[n_gram] += 1\n","      # Add n-gram count\n","      else:\n","        n_grams[n_gram] = 1\n","\n","  return n_grams"],"execution_count":10,"outputs":[]},{"metadata":{"id":"zEoawxT39vVR"},"cell_type":"markdown","source":["This function calculates the priority for the next word given the prior n-gram. This function also implements k-smoothing which helps account for unseen n-grams. Using the previously defined formula:\n","\n","\n","$$\\large\n","P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n)}{C(w_{n‚àíN+1:n‚àí1})}\n","$$\n","\n","---\n","\n","### K-smoothing\n","\n","But what if we come across a n-gram that wasn't in the training set. Then our denominator would would become zero and our definition of probability will become invalid. Thus, we use k-smoothing, which adds a positive constant $k$ to each numerator and $k \\times |V|$ in the denominator, where $|V|$ is the number of words in the vocabulary. This ensures any n-gram with zero count has the same probability of $\\frac{1}{|V|}$. Thus, our original estimation get's modified to:\n","\n","$$\\large\n","P(w_n|w_{n‚àíN+1:n‚àí1}) = \\frac{C(w_{n‚àíN+1:n‚àí1}w_n) + k}{C(w_{n‚àíN+1:n‚àí1} + k |V|)}\n","$$"]},{"metadata":{"trusted":true,"id":"VgvLJzOu9vVR","executionInfo":{"status":"ok","timestamp":1725206409637,"user_tz":-180,"elapsed":511,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def prob_for_single_word(word, previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary_size, k = 1.0) -> 'float':\n","\n","  # Convert the previous_n_gram into a tuple\n","  previous_n_gram = tuple(previous_n_gram)\n","\n","  # Calculating the count, if exists from our freq dictionary otherwise zero\n","  previous_n_gram_count = n_gram_counts[previous_n_gram] if previous_n_gram in n_gram_counts else 0\n","\n","  # The Denominator\n","  denom = previous_n_gram_count + k * vocabulary_size\n","\n","  # previous n-gram plus the current word as a tuple\n","  nplus1_gram = previous_n_gram + (word,)\n","\n","  # Calculating the nplus1 count, if exists from our freq dictionary otherwise zero\n","  nplus1_gram_count = nplus1_gram_counts[nplus1_gram] if nplus1_gram in nplus1_gram_counts else 0\n","\n","  # Numerator\n","  num = nplus1_gram_count + k\n","\n","  # Final Fraction\n","  prob = num / denom\n","  return prob"],"execution_count":11,"outputs":[]},{"metadata":{"id":"4W4ilE0N9vVT"},"cell_type":"markdown","source":["Now, we loop over all the words in the vocabulary and then compute their probabilites using our `prob_for_single_word()` fn."]},{"metadata":{"trusted":true,"id":"sq3NqDxW9vVT","executionInfo":{"status":"ok","timestamp":1725206438292,"user_tz":-180,"elapsed":7,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def probs(previous_n_gram, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0) -> 'dict':\n","\n","  # Convert to Tuple\n","  previous_n_gram = tuple(previous_n_gram)\n","\n","  # Add end and unknown tokens to the vocabulary\n","  vocabulary = vocabulary + [\"<e>\", \"<unk>\"]\n","\n","  # Calculate the size of the vocabulary\n","  vocabulary_size = len(vocabulary)\n","\n","  # Empty dict for probabilites\n","  probabilities = {}\n","\n","  # Iterate over words\n","  for word in vocabulary:\n","\n","    # Calculate probability\n","    probability = prob_for_single_word(word, previous_n_gram,\n","                                           n_gram_counts, nplus1_gram_counts,\n","                                           vocabulary_size, k=k)\n","    # Create mapping: word -> probability\n","    probabilities[word] = probability\n","\n","  return probabilities"],"execution_count":12,"outputs":[]},{"metadata":{"id":"bMH-l-eB9vVT"},"cell_type":"markdown","source":["<a id=\"auto-complete\"></a>\n","# üí¨ The Auto-Complete System"]},{"metadata":{"id":"nSmSNqzS9vVT"},"cell_type":"markdown","source":["Finally, we build our `auto_complete` fn. We simply loop over all the words in the vocabulary assuming that they can be the next word and then return the word with it's probability."]},{"metadata":{"trusted":true,"id":"XXhIlZp09vVT","executionInfo":{"status":"ok","timestamp":1725206442561,"user_tz":-180,"elapsed":511,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def auto_complete(previous_tokens, n_gram_counts, nplus1_gram_counts, vocabulary, k=1.0, start_with=None):\n","\n","\n","    # length of previous words\n","    n = len(list(n_gram_counts.keys())[0])\n","\n","    # most recent 'n' words\n","    previous_n_gram = previous_tokens[-n:]\n","\n","    # Calculate probabilty for all words\n","    probabilities = probs(previous_n_gram,n_gram_counts, nplus1_gram_counts,vocabulary, k=k)\n","\n","    # Intialize the suggestion and max probability\n","    suggestion = None\n","    max_prob = 0\n","\n","    # Iterate over all words and probabilites, returning the max.\n","    # We also add a check if the start_with parameter is provided\n","    for word, prob in probabilities.items():\n","\n","        if start_with != None:\n","\n","            if not word.startswith(start_with):\n","                continue\n","\n","        if prob > max_prob:\n","\n","            suggestion = word\n","            max_prob = prob\n","\n","    return suggestion, max_prob"],"execution_count":13,"outputs":[]},{"metadata":{"id":"-pXFbjn39vVU"},"cell_type":"markdown","source":["We can also loop over all the various n-gram models to get multiple suggestions. This function just extends from the previously defined function by **taking multiple n-gram counts** instead of one. This allows us to take unigram, bigram, .. counts into account as well."]},{"metadata":{"trusted":true,"id":"8WGAZLmL9vVU","executionInfo":{"status":"ok","timestamp":1725206446225,"user_tz":-180,"elapsed":557,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n","\n","    # See how many models we have\n","    count = len(n_gram_counts_list)\n","\n","    # Empty list for suggestions\n","    suggestions = []\n","\n","    # IMP: Earlier \"-1\"\n","\n","    # Loop over counts\n","    for i in range(count-1):\n","\n","        # get n and nplus1 counts\n","        n_gram_counts = n_gram_counts_list[i]\n","        nplus1_gram_counts = n_gram_counts_list[i+1]\n","\n","        # get suggestions\n","        suggestion = auto_complete(previous_tokens, n_gram_counts,\n","                                    nplus1_gram_counts, vocabulary,\n","                                    k=k, start_with=start_with)\n","        # Append to list\n","        suggestions.append(suggestion)\n","\n","    return suggestions"],"execution_count":14,"outputs":[]},{"metadata":{"id":"msoP7Nox9vVV"},"cell_type":"markdown","source":["<a id=\"inference\"></a>\n","# üòä Inference"]},{"metadata":{"id":"_PFIMQHf9vVV"},"cell_type":"markdown","source":["Here, we create a list of n-gram counts for a arbitrary range `(1,6)`"]},{"metadata":{"trusted":true,"id":"2g0Wy5H39vVW","executionInfo":{"status":"ok","timestamp":1725206565697,"user_tz":-180,"elapsed":116566,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"cell_type":"code","source":["n_gram_counts_list = []\n","for n in range(1, 6):\n","    n_model_counts = count_n_grams(final_train, n)\n","    n_gram_counts_list.append(n_model_counts)"],"execution_count":15,"outputs":[]},{"metadata":{"id":"5JAqKAWZ9vVX"},"cell_type":"markdown","source":["Let's give it a sample input of \"i was about\" in a tokenized manner and get multiple suggestions using the above calculated n-gram counts with smoothing-factor, `k` = 1.0"]},{"metadata":{"trusted":true,"id":"ZoABGLqj9vVY"},"cell_type":"code","source":["previous_tokens = [\"i\", \"was\", \"about\"]\n","suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n","\n","display(suggestion)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["previous_tokens = [\"i\", \"am\", \"learning\"]\n","suggestion = get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0)\n","\n","display(suggestion)"],"metadata":{"id":"vQN9qFrkVB0g"},"execution_count":null,"outputs":[]},{"metadata":{"id":"3n-68U7-9vVc"},"cell_type":"markdown","source":["[Source](https://www.kaggle.com/code/sauravmaheshkar/auto-completion-using-n-gram-models) for this tutorial."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}