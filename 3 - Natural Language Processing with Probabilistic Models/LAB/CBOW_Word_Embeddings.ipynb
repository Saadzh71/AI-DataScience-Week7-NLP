{"cells":[{"cell_type":"markdown","id":"90fe34d3","metadata":{"id":"90fe34d3"},"source":["# Tutorial: Word Embeddings using CBOW\n","In this tutorial, we'll learn how to create word embeddings using the Continuous Bag of Words (CBOW) model. Word embeddings are vector representations of words that capture their meanings, contexts, and relationships. CBOW is a simple and effective neural network model that predicts a target word from its surrounding context words."]},{"cell_type":"markdown","id":"8cc7b9e4","metadata":{"id":"8cc7b9e4"},"source":["## Step 1: Importing Required Libraries\n"]},{"cell_type":"code","execution_count":17,"id":"3db9a09f","metadata":{"id":"3db9a09f","executionInfo":{"status":"ok","timestamp":1725212247707,"user_tz":-180,"elapsed":493,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"outputs":[],"source":["import tensorflow as tf\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.layers import Dense, Embedding, Lambda\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.decomposition import PCA"]},{"cell_type":"markdown","id":"dad03726","metadata":{"id":"dad03726"},"source":["## Step 2: Defining the Corpus\n","Let's define a simple corpus of sentences to train our CBOW model. This corpus will consist of three sentences."]},{"cell_type":"code","execution_count":18,"id":"14f795b7","metadata":{"id":"14f795b7","executionInfo":{"status":"ok","timestamp":1725212249663,"user_tz":-180,"elapsed":6,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"outputs":[],"source":["corpus = ['The cat sat on the mat',\n","          'The dog ran in the park',\n","          'The bird sang in the tree']"]},{"cell_type":"markdown","id":"700a9092","metadata":{"id":"700a9092"},"source":["## Step 3: Tokenizing the Corpus\n","We convert the corpus into a sequence of integers using Keras' `Tokenizer`. This step is necessary to transform the text data into numerical data that can be used by the neural network."]},{"cell_type":"code","execution_count":null,"id":"85bd177d","metadata":{"id":"85bd177d"},"outputs":[],"source":["tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(corpus)\n","sequences = tokenizer.texts_to_sequences(corpus)\n","print('After converting our words in the corpus into vector of integers:')\n","print(sequences)"]},{"cell_type":"markdown","id":"95b2066b","metadata":{"id":"95b2066b"},"source":["## Step 4: Defining Model Parameters\n","Next, we define some parameters for our CBOW model:\n","- `vocab_size`: The size of the vocabulary (total number of unique words).\n","- `embedding_size`: The size of the word embedding vectors.\n","- `window_size`: The number of context words to consider on either side of the target word."]},{"cell_type":"code","execution_count":20,"id":"b4d5a81e","metadata":{"id":"b4d5a81e","executionInfo":{"status":"ok","timestamp":1725212261048,"user_tz":-180,"elapsed":349,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"outputs":[],"source":["vocab_size = len(tokenizer.word_index) + 1\n","embedding_size = 10\n","window_size = 2"]},{"cell_type":"markdown","id":"5996dc9a","metadata":{"id":"5996dc9a"},"source":["## Step 5: Generating Context-Target Pairs\n","We generate context-target pairs for training the CBOW model. The context consists of words surrounding a target word within a defined window size."]},{"cell_type":"code","execution_count":21,"id":"833164b4","metadata":{"id":"833164b4","executionInfo":{"status":"ok","timestamp":1725212263736,"user_tz":-180,"elapsed":355,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"outputs":[],"source":["contexts = []\n","targets = []\n","for sequence in sequences:\n","    for i in range(window_size, len(sequence) - window_size):\n","        context = sequence[i - window_size:i] + sequence[i + 1:i + window_size + 1]\n","        target = sequence[i]\n","        contexts.append(context)\n","        targets.append(target)\n","X = np.array(contexts)\n","y = tf.keras.utils.to_categorical(targets, num_classes=vocab_size)"]},{"cell_type":"markdown","id":"1ede0309","metadata":{"id":"1ede0309"},"source":["## Step 6: Building the CBOW Model\n","We use Keras' Sequential API to build the CBOW model. The model consists of an `Embedding` layer to learn word embeddings, a `Lambda` layer to average the embeddings of context words, and a `Dense` layer with a softmax activation function to predict the target word."]},{"cell_type":"code","execution_count":null,"id":"957b3d6d","metadata":{"id":"957b3d6d"},"outputs":[],"source":["model = Sequential()\n","model.add(Embedding(input_dim=vocab_size, output_dim=embedding_size, input_length=2*window_size))\n","model.add(Lambda(lambda x: tf.reduce_mean(x, axis=1)))\n","model.add(Dense(units=vocab_size, activation='softmax'))\n","model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X, y, epochs=100, verbose=1)\n","model.save_weights('cbow_model.weights.h5')"]},{"cell_type":"markdown","id":"975468c5","metadata":{"id":"975468c5"},"source":["## Step 7: Loading Pre-trained Weights and Extracting Word Embeddings\n","We load the pre-trained weights and extract the word embeddings from the model."]},{"cell_type":"code","execution_count":23,"id":"21504a3b","metadata":{"id":"21504a3b","executionInfo":{"status":"ok","timestamp":1725212276253,"user_tz":-180,"elapsed":336,"user":{"displayName":"Badr AI (T5_BKM)","userId":"05416374041059952227"}}},"outputs":[],"source":["model.load_weights('cbow_model.weights.h5')\n","embeddings = model.get_weights()[0]"]},{"cell_type":"markdown","id":"994b0780","metadata":{"id":"994b0780"},"source":["## Step 8: Visualizing Word Embeddings\n","To visualize the word embeddings, we use Principal Component Analysis (PCA) to reduce their dimensionality to 2D. This allows us to plot the embeddings on a 2D plane."]},{"cell_type":"code","execution_count":null,"id":"829098b1","metadata":{"id":"829098b1"},"outputs":[],"source":["pca = PCA(n_components=2)\n","reduced_embeddings = pca.fit_transform(embeddings)\n","plt.figure(figsize=(5, 5))\n","for i, word in enumerate(tokenizer.word_index.keys()):\n","    x, y = reduced_embeddings[i]\n","    plt.scatter(x, y)\n","    plt.annotate(word, xy=(x, y), xytext=(5, 2), textcoords='offset points', ha='right', va='bottom')\n","plt.show()"]},{"cell_type":"markdown","source":["[Source](https://www.geeksforgeeks.org/continuous-bag-of-words-cbow-in-nlp/) for this tutorial."],"metadata":{"id":"Pcx3bE_Hd-A5"},"id":"Pcx3bE_Hd-A5"}],"metadata":{"colab":{"provenance":[]},"language_info":{"name":"python"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"nbformat":4,"nbformat_minor":5}