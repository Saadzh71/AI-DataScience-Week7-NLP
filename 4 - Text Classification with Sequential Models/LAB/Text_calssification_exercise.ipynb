{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "1ad31a0f",
      "metadata": {
        "id": "1ad31a0f"
      },
      "source": [
        "# Text Classification with 20 Newsgroups Dataset using RNN and LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ebc80472",
      "metadata": {
        "id": "ebc80472"
      },
      "source": [
        "This notebook demonstrates how to perform text classification using both RNN and LSTM models on the 20 Newsgroups dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "db322b61",
      "metadata": {
        "id": "db322b61"
      },
      "source": [
        "### Step 1: Install TensorFlow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "b8ec59a9",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b8ec59a9",
        "outputId": "eac3ce3f-6bf2-41a3-ba27-6c7b2f3ce2c1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.17.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.11.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.1)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (71.0.4)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.64.1)\n",
            "Requirement already satisfied: tensorboard<2.18,>=2.17 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.17.0)\n",
            "Requirement already satisfied: keras>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.44.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (13.8.0)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.10/dist-packages (from keras>=3.2.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorflow) (2024.7.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.18,>=2.17->tensorflow) (3.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.18,>=2.17->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->keras>=3.2.0->tensorflow) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.2.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eab6d57e",
      "metadata": {
        "id": "eab6d57e"
      },
      "source": [
        "### Step 2: Import Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "a2cbd035",
      "metadata": {
        "id": "a2cbd035"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, SimpleRNN, LSTM, Dense\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "09ada53d",
      "metadata": {
        "id": "09ada53d"
      },
      "source": [
        "### Step 3: Load and Preprocess the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "3a87334b",
      "metadata": {
        "id": "3a87334b"
      },
      "outputs": [],
      "source": [
        "# Load the 20 Newsgroups dataset\n",
        "newsgroups = fetch_20newsgroups(subset='all', shuffle=True, random_state=42)\n",
        "\n",
        "# Preprocess the data\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(newsgroups.data)\n",
        "sequences = tokenizer.texts_to_sequences(newsgroups.data)\n",
        "X = pad_sequences(sequences, maxlen=200)\n",
        "y = to_categorical(newsgroups.target)\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01298bbc",
      "metadata": {
        "id": "01298bbc"
      },
      "source": [
        "### Step 4: Build and Compile the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "532a3f98",
      "metadata": {
        "id": "532a3f98"
      },
      "outputs": [],
      "source": [
        "# Build the RNN model\n",
        "rnn_model = Sequential()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bc1e2a87",
      "metadata": {
        "id": "bc1e2a87"
      },
      "source": [
        "### Step 5: Build and Compile the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5c2553ec",
      "metadata": {
        "id": "5c2553ec"
      },
      "outputs": [],
      "source": [
        "# Build the LSTM model\n",
        "lstm_model = Sequential()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fed0ef0f",
      "metadata": {
        "id": "fed0ef0f"
      },
      "source": [
        "### Step 6: Train the RNN Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22dd147c",
      "metadata": {
        "id": "22dd147c"
      },
      "outputs": [],
      "source": [
        "# Train the RNN model\n",
        "history_rnn ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "aa242b75",
      "metadata": {
        "id": "aa242b75"
      },
      "source": [
        "### Step 7: Train the LSTM Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cfd0f492",
      "metadata": {
        "id": "cfd0f492"
      },
      "outputs": [],
      "source": [
        "# Train the LSTM model\n",
        "history_lstm ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1583f324",
      "metadata": {
        "id": "1583f324"
      },
      "source": [
        "### Step 8: Evaluate the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "838b1e90",
      "metadata": {
        "id": "838b1e90"
      },
      "outputs": [],
      "source": [
        "# Evaluate the models\n",
        "rnn_eval =\n",
        "lstm_eval ="
      ]
    },
    {
      "cell_type": "markdown",
      "id": "97dfebf9",
      "metadata": {
        "id": "97dfebf9"
      },
      "source": [
        "### Step 9: Plot the Training and Validation Accuracy/Loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "672d62ac",
      "metadata": {
        "id": "672d62ac"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "c801c583",
      "metadata": {
        "id": "c801c583"
      },
      "source": [
        "### Step 10: Classify a Sample Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a613e1b",
      "metadata": {
        "id": "1a613e1b"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#**Well done =)**"
      ],
      "metadata": {
        "id": "i2xRJx2pF9OX"
      },
      "id": "i2xRJx2pF9OX"
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}