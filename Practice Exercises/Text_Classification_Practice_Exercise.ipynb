{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Important Notice! ðŸš¨"
      ],
      "metadata": {
        "id": "spruvyDmgE-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "While this dataset might seem relatively clean, be prepared! In the actual exam, you may encounter a more challenging dataset containing URLs, emojis, Tashkeel, last Harakah, and other elements that will require proper handling.\n",
        "\n",
        "Donâ€™t rely on this clean dataset alone! Itâ€™s highly recommended that you practice using a different, more complex dataset. Handling these types of noise effectively will not only help you excel in the exam but also prepare you for real-world projects involving Arabic text data.\n",
        "\n",
        "Here are a few things you might need to clean in other datasets:\n",
        "- URLs\n",
        "- Emojis\n",
        "- Arabic diacritics (Tashkeel) and last Harakah\n",
        "\n",
        "**Take Your Time**: The more you practice with noisy data, the better equipped you'll be to tackle these challenges confidently, both in the exam and in future projects! ðŸš€\n"
      ],
      "metadata": {
        "id": "haXAxpazgHm_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Install and Import Needed Libraries"
      ],
      "metadata": {
        "id": "oqw2El5mMxsz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can use `pyarabic` or any other library to pre-process and clean the Arabic text."
      ],
      "metadata": {
        "id": "4dTkHbSHNB4y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z6mRe6i5Wqqc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download the Dataset"
      ],
      "metadata": {
        "id": "Hq5KJRZ5Ph0Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d abedkhooli/arabic-100k-reviews"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fa9DlT5K_QB7",
        "outputId": "96dbdb10-461e-494b-95e0-f13a6bcb8022"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/abedkhooli/arabic-100k-reviews\n",
            "License(s): copyright-authors\n",
            "Downloading arabic-100k-reviews.zip to /content\n",
            " 29% 5.00M/17.0M [00:00<00:00, 31.2MB/s]\n",
            "100% 17.0M/17.0M [00:00<00:00, 81.3MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!unzip arabic-100k-reviews.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-xxSfZC0BOqY",
        "outputId": "ee11c0a5-f5d3-4bf7-8c73-0538eaac687e"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  arabic-100k-reviews.zip\n",
            "  inflating: ar_reviews_100k.tsv     \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Exploration"
      ],
      "metadata": {
        "id": "kVc0b_9fQGeH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Before diving into preprocessing and model building, itâ€™s important to first explore the dataset to understand its structure, distribution, and key characteristics. This step will help you gain insights into the data and guide your decisions in subsequent steps. Hereâ€™s what to consider:\n",
        "\n",
        "1. **Inspect the Data**:\n",
        "   Start by looking at the first few rows of the dataset to get a sense of its structure. Check the columns, data types, and a few sample entries. This helps to ensure that the data is loaded correctly and gives you an initial overview of the content.\n",
        "\n",
        "2. **Check for Missing Values**:\n",
        "   Identify if there are any missing values in the dataset. Missing data can cause issues during model training, so itâ€™s important to handle them early on. You can choose to remove rows with missing values or fill them with appropriate replacements (imputation).\n",
        "\n",
        "3. **Distribution of Labels**:\n",
        "   Examine the distribution of the target labels (classes). This step is crucial in classification tasks to determine if the dataset is balanced or imbalanced. A balanced dataset has a roughly equal number of samples for each class, while an imbalanced dataset has a significantly higher number of samples for one class compared to others. For imbalanced datasets, you may need to apply techniques like class weighting or data augmentation.\n",
        "\n",
        "4. **Text Data Characteristics**:\n",
        "   Analyze the length of the text data. It is useful to calculate the number of words or characters in each text sample to understand how long the texts are. This will help you set a suitable `max_length` for tokenization and padding later. You can plot a histogram of text lengths to visualize the distribution.\n",
        "\n",
        "5. **Common Words and Vocabulary**:\n",
        "   Explore the most frequent words in the text data. This will give you a sense of which words dominate the dataset and could be helpful in later steps, such as stopword removal or adjusting tokenizer settings."
      ],
      "metadata": {
        "id": "a98DaVUvQsi8"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aZgYQbZxWtAt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Cleaning and Preprocessing"
      ],
      "metadata": {
        "id": "X_oEu0TkSMEB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section, we will focus on cleaning and filtering the dataset, preparing it for the text classification task. We will implement the following steps:\n",
        "\n",
        "1. **Remove missing values**:\n",
        "   First, we eliminate any rows with missing values to ensure the dataset is complete and consistent.\n",
        "\n",
        "2. **Filter by text length**:\n",
        "   To maintain a uniform dataset, we will filter the text samples by a specified word count range. This ensures that the texts are neither too short to lack context nor too long to introduce unnecessary complexity.\n",
        "\n",
        "3. **Exclude 'Mixed' labels**:\n",
        "   If the dataset contains any 'Mixed' labels, indicating ambiguous or unclear classification, we will remove these entries for better model accuracy.\n",
        "\n",
        "4. **Arabic stopwords loading**:\n",
        "   We load a list of Arabic stopwords to filter out commonly used but contextually insignificant words. This is an important step for improving the performance of the model, as stopwords do not contribute valuable information.\n",
        "\n",
        "5. **Text cleaning**:\n",
        "   We apply a series of text cleaning steps to standardize and simplify the text data. This involves:\n",
        "   \n",
        "   - **Removing links (URLs)**: Any URLs present in the text are removed as they are not meaningful for classification purposes.\n",
        "   - **Removing special characters and punctuation**: This step removes any non-alphabetical characters, ensuring the text only contains meaningful words.\n",
        "   - **Removing Arabic diacritics (Tashkeel) and elongated letters (Tatweel)**: Diacritical marks and elongated letters are stripped out to standardize the text.\n",
        "   - **Removing Arabic stopwords**: Words that are part of the stopwords list are removed, as they do not add value to the classification task.\n",
        "   - **Stemming**: Each word is reduced to its root form using an Arabic stemmer (ISRIStemmer) to simplify the vocabulary.\n",
        "   - **Normalizing Hamza**: Any variation of the Hamza character is normalized for consistency.\n",
        "\n",
        "6. **Final cleanup**:\n",
        "   Apply the cleanup function to the feature column.\n",
        "\n",
        "By following these steps, the text will be cleaned, filtered, and ready for tokenization!"
      ],
      "metadata": {
        "id": "wSAFiozlRQHt"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mHEObOY_fHhq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization, Padding, and Data Splitting"
      ],
      "metadata": {
        "id": "VF45GS_ZSEix"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, we will prepare the text data for input into a machine learning model by converting the text into numerical sequences, padding them to a uniform length, and splitting the dataset into training and testing sets. Here's an overview of the steps involved:\n",
        "\n",
        "1. **Tokenization**:\n",
        "   We use a tokenizer to convert the cleaned text into numerical sequences. Each word in the text is assigned a unique integer ID based on the frequency of its occurrence in the dataset. This allows the text to be represented numerically, which is necessary for model training.\n",
        "\n",
        "2. **Text to sequences**:\n",
        "   After fitting the tokenizer on the cleaned text, we transform each text into a sequence of numbers, where each number corresponds to a token (word) in the text. These sequences represent the entire dataset in numerical form.\n",
        "\n",
        "3. **Padding the sequences**:\n",
        "   Since different texts may vary in length, we pad the sequences to ensure they all have the same length. Padding adds zeroes at the end (or beginning) of each sequence until they reach the defined maximum length. This is important for creating uniform input data for the model.\n",
        "\n",
        "4. **Label encoding**:\n",
        "   The labels (target values) also need to be converted into numerical form. We use a label encoder to transform the categorical labels into numerical representations. This step is essential because machine learning models work with numerical data.\n",
        "\n",
        "5. **Train-test split**:\n",
        "   The dataset is divided into training and testing sets. We allocate 80% of the data for training the model and reserve 20% for testing its performance. This split ensures that we have a portion of the data to evaluate how well the model generalizes to unseen data.\n",
        "   \n",
        "   - The **training data** consists of the padded sequences used to train the model.\n",
        "   - The **training labels** are the encoded labels corresponding to the training data.\n",
        "   - The **testing data** is used to assess the modelâ€™s performance after training.\n",
        "   - The **testing labels** are the encoded labels corresponding to the testing data.\n",
        "\n",
        "6. **Data shape confirmation**:\n",
        "   After splitting the data, we print the shape (dimensions) of both the training and testing sets to confirm that the data is properly divided and formatted.\n",
        "\n",
        "By the end of this step, the text data will be transformed into padded numerical sequences, the labels will be encoded, and the data will be split into training and testing sets for model development and evaluation."
      ],
      "metadata": {
        "id": "YmsL3S-5SIbl"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z401Re0VfI1f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Building the RNN Model"
      ],
      "metadata": {
        "id": "qd5Ek4NLTgVN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, you will design and build a Recurrent Neural Network (RNN) model to classify text data. Below is a breakdown of the key components you'll implement, but it's up to you to decide how to configure them based on your understanding and experimentation:\n",
        "\n",
        "1. **Model Type**:\n",
        "   You will use a Sequential model, which allows you to stack layers in a linear sequence. Feel free to explore different model types if necessary.\n",
        "\n",
        "2. **Input Layer**:\n",
        "   Define the shape of the input data. Consider the dimensions of your padded sequences and set the input shape accordingly.\n",
        "\n",
        "3. **Embedding Layer**:\n",
        "   The embedding layer will convert input tokens (integers) into dense vector representations. You will need to determine the size of the input dimension (based on your vocabulary) and the output dimension (embedding size). The embedding size is a hyperparameter, so feel free to experiment with different values.\n",
        "\n",
        "4. **Bidirectional Simple RNN Layers**:\n",
        "   You can add one or more recurrent layers. Consider using Bidirectional layers to capture contextual information from both directions (forward and backward). You have the option to specify the number of units in each layer. Additionally, you can decide whether to return sequences from the layers and how many layers to stack.\n",
        "\n",
        "5. **Dense Layers**:\n",
        "   Add one or more fully connected (Dense) layers to process the output from the RNN layers. You will need to choose the number of units for these layers and decide on an activation function. ReLU is a common choice, but feel free to explore other options depending on your task.\n",
        "\n",
        "6. **Output Layer**:\n",
        "   The output layer should match the type of classification task you're working on. For a binary classification task, consider using a single unit with an appropriate activation function (e.g., sigmoid). If you're working on a multi-class classification, a different setup may be required.\n",
        "\n",
        "7. **Model Summary**:\n",
        "   After defining your model architecture, print a summary to review the number of layers, types of layers, and total parameters.\n",
        "\n",
        "8. **Model Compilation**:\n",
        "   Finally, compile the model by selecting an optimizer, a loss function, and metrics. Common choices include Adam for the optimizer and binary cross-entropy for the loss function in binary classification, but this is entirely up to you. You can experiment with different optimizers and loss functions to see what works best for your specific problem."
      ],
      "metadata": {
        "id": "HpLEoTaITEGx"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "D-EYYIUpfK_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Defining Batch Size, Creating Datasets, and Training the Model"
      ],
      "metadata": {
        "id": "57DXwqqQTloy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, you will define the batch size, create TensorFlow Datasets for both training and testing, and train the model. The key elements to consider are outlined below, and it is up to you to choose the specific configurations based on your preferences and experimentation:\n",
        "\n",
        "1. **Batch Size**:\n",
        "   Select a batch size for training and testing. The batch size determines how many samples will be processed together in one forward and backward pass during training. Smaller batch sizes may allow for more granular updates, while larger batch sizes might lead to faster but less frequent updates. You can experiment with different values.\n",
        "\n",
        "2. **Creating Datasets**:\n",
        "   Use TensorFlowâ€™s `Dataset.from_tensor_slices()` to create datasets from the training and testing data. This step converts the feature data and labels into pairs of tensors, which can be efficiently fed into the model during training and testing.\n",
        "\n",
        "3. **Batching the Datasets**:\n",
        "   Batch the datasets by grouping the data into batches of the specified size. This is important for ensuring that the model processes the data in manageable chunks during training. You can choose the batch size that works best for your hardware and data.\n",
        "\n",
        "4. **Training the Model**:\n",
        "   Train the model by fitting it on the training dataset for a specified number of epochs. You will also need to provide the validation data (the testing dataset in this case) to monitor the modelâ€™s performance on unseen data during training. The number of epochs is another hyperparameter you can adjust, with higher values potentially leading to better performance but risking overfitting.\n",
        "\n",
        "5. **Tracking Training History**:\n",
        "   During training, the modelâ€™s performance metrics (such as loss and accuracy) will be tracked over the epochs, and the results will be stored in the `history` object. You can use this to analyze and visualize how the modelâ€™s performance evolves over time."
      ],
      "metadata": {
        "id": "LEOvs_dETmQp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o6unhJgFfQbM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Evaluation"
      ],
      "metadata": {
        "id": "XmoJfr3nfP-n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once the model is trained, the next step is to evaluate its performance on the testing dataset. This will give you a sense of how well the model generalizes to unseen data. Hereâ€™s an overview of what you need to consider for this step:\n",
        "\n",
        "1. **Evaluate the Model**:\n",
        "   You will use the `evaluate()` method to assess the modelâ€™s performance on the test dataset. This method calculates the loss and other metrics (e.g., accuracy) that you specified during model compilation. It provides a final measure of how well the model performs on data it hasnâ€™t seen during training.\n",
        "\n",
        "2. **Testing Dataset**:\n",
        "   Ensure that the testing dataset is properly prepared and batched, just like the training dataset. The evaluation process will loop through the batches and compute the overall performance across the entire test set.\n",
        "\n",
        "3. **Metrics**:\n",
        "   The metrics used during evaluation are the same as those defined in the modelâ€™s compilation step. Common metrics include accuracy for classification tasks, but feel free to define and track any other metrics of interest.\n",
        "\n",
        "4. **Loss Curve**:\n",
        "   A loss curve plots the loss values for both the training and validation datasets over the epochs. This visualization helps to monitor the modelâ€™s performance during training. A decreasing training loss usually indicates that the model is learning, while the validation loss shows how well the model generalizes to unseen data."
      ],
      "metadata": {
        "id": "2Gb8G9XiT-ec"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SdECXvQGUQae"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model Inference"
      ],
      "metadata": {
        "id": "ifx5RFxnUQqc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this step, you will use the trained model to make predictions on new, unseen data (inference). Hereâ€™s an outline of the key points:\n",
        "\n",
        "1. **Prepare New Data**:\n",
        "   Before making predictions, ensure that the new data is preprocessed in the same way as the training data. This includes tokenization, padding, and any other transformations you applied during the data preprocessing step. The data can be single text to see the result of the prediction.\n",
        "\n",
        "2. **Model Prediction**:\n",
        "   Use the `predict()` method to feed new samples into the trained model and obtain predictions. The model will output probabilities or predicted class labels based on the type of classification task (binary or multi-class).\n",
        "\n",
        "3. **Interpreting Predictions**:\n",
        "   The model will return probabilities for each class. For binary classification, the output will typically be a probability value between 0 and 1."
      ],
      "metadata": {
        "id": "sVh0WCBoUSlP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2XiahrhffR0J"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}